{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e23e86d",
   "metadata": {},
   "source": [
    "# 🧠 GRU (Gated Recurrent Unit)\n",
    "\n",
    "## Definition\n",
    "GRU is a type of neural network used for understanding and working with sequences — like sentences, time series, or audio.\n",
    "\n",
    "It remembers important information and forgets unimportant parts using two smart \"gates\":\n",
    "\n",
    "- **Update Gate** – decides how much old memory to keep  \n",
    "- **Reset Gate** – decides how much past info to forget  \n",
    "\n",
    "GRU is faster and simpler than LSTM, but still very good at handling long-term information.\n",
    "\n",
    "✅ **In short**: GRU helps computers understand data that comes in order (like text or time), by remembering what's important and forgetting what's not.\n",
    "\n",
    "---\n",
    "\n",
    "## 🕰️ History of GRU\n",
    "\n",
    "| Year | Event |\n",
    "|------|-------|\n",
    "| 1991 | RNN introduced (simple memory over time) |\n",
    "| 1997 | LSTM introduced (solved forgetting problem) |\n",
    "| 2014 | GRU introduced by Kyunghyun Cho |\n",
    "\n",
    "GRU was made to be simpler and faster than LSTM, while still solving the problems of RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Why GRU Was Needed\n",
    "\n",
    "### 🧠 Problem with RNN:\n",
    "- Forgets long-term information\n",
    "- Suffers from vanishing gradient problem (it stops learning)\n",
    "- Not good for long sentences or time-series data\n",
    "\n",
    "### ✅ Solution: LSTM and GRU\n",
    "- Both are improved versions of RNN\n",
    "- GRU is simpler than LSTM but works nearly as well\n",
    "- GRU is faster to train (fewer parts)\n",
    "\n",
    "---\n",
    "\n",
    "## � GRU Architecture – In Simple Words\n",
    "\n",
    "Imagine GRU as a smart box with:\n",
    "\n",
    "| Part | Purpose | Emoji |\n",
    "|------|---------|-------|\n",
    "| Update Gate | Should I keep old memory or add new? | 🔁 |\n",
    "| Reset Gate | Should I forget the past memory? | 🔄 |\n",
    "| Hidden State | This is the memory carried to the next step | 🧠 |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 How GRU Works (Step by Step):\n",
    "1. It looks at the current input (like a word or number)\n",
    "2. It also checks the previous memory\n",
    "3. Reset gate decides: \"Should I forget the old memory?\"\n",
    "4. Update gate decides: \"Should I keep old memory or add new?\"\n",
    "5. It mixes the old and new information to create the final memory\n",
    "6. This memory is passed to the next time step\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 GRU vs LSTM vs RNN – Easy Comparison\n",
    "\n",
    "| Feature | RNN | LSTM | GRU |\n",
    "|---------|-----|------|-----|\n",
    "| Memory Ability | Weak (forgets easily) | Strong (remembers long-term) | Strong (like LSTM) |\n",
    "| Gates Used | None | 3 gates (input, forget, output) | 2 gates (update, reset) |\n",
    "| Cell State | No | Yes (cell + hidden) | No (only hidden state) |\n",
    "| Speed | Fast | Slow | Faster than LSTM |\n",
    "| Simplicity | Simple | Complex | Medium (simpler than LSTM) |\n",
    "| Best For | Short sequences | Long and complex sequences | Long sequences, faster tasks |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 GRU Architecture Diagram\n",
    "\n",
    "```plaintext\n",
    "        ┌──────────────┐\n",
    "        │ Prev hidden  │\n",
    "        └─────┬────────┘\n",
    "              │\n",
    "        ┌─────▼────────┐\n",
    "        │ Update Gate  │ ← Should I keep old memory?\n",
    "        └─────┬────────┘\n",
    "              │\n",
    "        ┌─────▼────────┐\n",
    "        │ Reset Gate   │ ← Should I forget past?\n",
    "        └─────┬────────┘\n",
    "              │\n",
    "        ┌─────▼────────┐\n",
    "        │ Candidate h~ │ ← New memory (with reset info)\n",
    "        └─────┬────────┘\n",
    "              │\n",
    "        ┌─────▼────────┐\n",
    "        │  Final h(t)  │ ← Combined output (new + old)\n",
    "        └──────────────┘\n",
    "```\n",
    "\n",
    "## 🧪 GRU Gates and Equations\n",
    "\n",
    "### 🔁 1. Update Gate – How much to update memory?\n",
    "Controls how much of the previous hidden state to keep.\n",
    "\n",
    "```\n",
    "zₜ = σ(W_z⋅[hₜ₋₁, xₜ] + b_z)\n",
    "\n",
    "Expend or Explain the Formula\n",
    "\n",
    "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "```\n",
    "- If zₜ ≈ 1: Keep old memory (maintain previous state)\n",
    "- If zₜ ≈ 0: Use new memory (update with current input)\n",
    "\n",
    "### 🔄 2. Reset Gate – How much past to forget?\n",
    "Controls how much of the past to ignore.\n",
    "```\n",
    "rₜ = σ(W_r⋅[hₜ₋₁, xₜ] + b_r)\n",
    "\n",
    "Expend or Explain the Formula\n",
    "\n",
    "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "```\n",
    "- If rₜ ≈ 0: Forget past\n",
    "- If rₜ ≈ 1: Remember all past\n",
    "\n",
    "### 🧠 3. Candidate Hidden State – New memory to consider\n",
    "\n",
    "```\n",
    "h̃ₜ = tanh(W_h⋅[rₜ∗hₜ₋₁, xₜ] + b_h)\n",
    "\n",
    "```\n",
    "Combines current input and past memory  \n",
    "Adjusted by reset gate\n",
    "\n",
    "### 📤 Final Hidden State – The updated memory/output\n",
    "\n",
    "```\n",
    "hₜ = (1−zₜ)∗hₜ₋₁ + zₜ∗h̃ₜ\n",
    "```\n",
    "\n",
    "Mixes old memory and new memory  \n",
    "Controlled by update gate\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Full Flow of GRU at Time Step t\n",
    "1. Take current input xₜ and previous output hₜ₋₁\n",
    "2. Compute update gate zₜ\n",
    "3. Compute reset gate rₜ\n",
    "4. Calculate new memory h̃ₜ\n",
    "5. Combine using zₜ to get new hidden state hₜ\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Real-Life Example (To Understand GRU)\n",
    "Imagine you're watching a TV drama series:\n",
    "- You remember main storylines (update gate keeps useful info)\n",
    "- You forget boring details like what color clothes someone wore (reset gate forgets unimportant parts)\n",
    "- Every new episode adds to your memory without confusing you\n",
    "\n",
    "That's how GRU works!\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Where GRU is Used\n",
    "\n",
    "| Use Case | What GRU Does |\n",
    "|----------|---------------|\n",
    "| Language Translation | Remembers sentence structure |\n",
    "| Sentiment Analysis | Understands meaning in reviews |\n",
    "| Time Series Forecasting | Predicts future stock/weather |\n",
    "| Chatbots | Keeps conversation flow |\n",
    "| Speech Recognition | Understands spoken words over time |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Final Summary – GRU in One Shot\n",
    "✅ GRU is a smart RNN that learns what to remember and what to forget  \n",
    "🔁 Uses two gates (Update & Reset)  \n",
    "🧠 Keeps a single hidden state (no cell state like LSTM)  \n",
    "⚡ Works well for long sequences and is faster than LSTM  \n",
    "💬 Perfect for tasks like text, speech, and time series\n",
    "\n",
    "## **✅ GRU Model (Single Layer) with IMDB Dataset in Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896eb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1. Load the IMDB Dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# 2. Pad Sequences to have the same length\n",
    "x_train = pad_sequences(x_train, maxlen=100)\n",
    "x_test = pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "# 3. Define the GRU Model (Single Layer)\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),  # Converts words to vectors\n",
    "    GRU(32),                                 # Single GRU Layer\n",
    "    Dense(1, activation='sigmoid')           # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# 4. Compile the Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 5. Model Summary\n",
    "model.summary()\n",
    "\n",
    "# 6. Train the Model\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
