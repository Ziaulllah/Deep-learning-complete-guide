{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e23e86d",
   "metadata": {},
   "source": [
    "# ğŸ§  GRU (Gated Recurrent Unit)\n",
    "\n",
    "## Definition\n",
    "GRU is a type of neural network used for understanding and working with sequences â€” like sentences, time series, or audio.\n",
    "\n",
    "It remembers important information and forgets unimportant parts using two smart \"gates\":\n",
    "\n",
    "- **Update Gate** â€“ decides how much old memory to keep  \n",
    "- **Reset Gate** â€“ decides how much past info to forget  \n",
    "\n",
    "GRU is faster and simpler than LSTM, but still very good at handling long-term information.\n",
    "\n",
    "âœ… **In short**: GRU helps computers understand data that comes in order (like text or time), by remembering what's important and forgetting what's not.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ•°ï¸ History of GRU\n",
    "\n",
    "| Year | Event |\n",
    "|------|-------|\n",
    "| 1991 | RNN introduced (simple memory over time) |\n",
    "| 1997 | LSTM introduced (solved forgetting problem) |\n",
    "| 2014 | GRU introduced by Kyunghyun Cho |\n",
    "\n",
    "GRU was made to be simpler and faster than LSTM, while still solving the problems of RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Why GRU Was Needed\n",
    "\n",
    "### ğŸ§  Problem with RNN:\n",
    "- Forgets long-term information\n",
    "- Suffers from vanishing gradient problem (it stops learning)\n",
    "- Not good for long sentences or time-series data\n",
    "\n",
    "### âœ… Solution: LSTM and GRU\n",
    "- Both are improved versions of RNN\n",
    "- GRU is simpler than LSTM but works nearly as well\n",
    "- GRU is faster to train (fewer parts)\n",
    "\n",
    "---\n",
    "\n",
    "## ï¿½ GRU Architecture â€“ In Simple Words\n",
    "\n",
    "Imagine GRU as a smart box with:\n",
    "\n",
    "| Part | Purpose | Emoji |\n",
    "|------|---------|-------|\n",
    "| Update Gate | Should I keep old memory or add new? | ğŸ” |\n",
    "| Reset Gate | Should I forget the past memory? | ğŸ”„ |\n",
    "| Hidden State | This is the memory carried to the next step | ğŸ§  |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ How GRU Works (Step by Step):\n",
    "1. It looks at the current input (like a word or number)\n",
    "2. It also checks the previous memory\n",
    "3. Reset gate decides: \"Should I forget the old memory?\"\n",
    "4. Update gate decides: \"Should I keep old memory or add new?\"\n",
    "5. It mixes the old and new information to create the final memory\n",
    "6. This memory is passed to the next time step\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª GRU vs LSTM vs RNN â€“ Easy Comparison\n",
    "\n",
    "| Feature | RNN | LSTM | GRU |\n",
    "|---------|-----|------|-----|\n",
    "| Memory Ability | Weak (forgets easily) | Strong (remembers long-term) | Strong (like LSTM) |\n",
    "| Gates Used | None | 3 gates (input, forget, output) | 2 gates (update, reset) |\n",
    "| Cell State | No | Yes (cell + hidden) | No (only hidden state) |\n",
    "| Speed | Fast | Slow | Faster than LSTM |\n",
    "| Simplicity | Simple | Complex | Medium (simpler than LSTM) |\n",
    "| Best For | Short sequences | Long and complex sequences | Long sequences, faster tasks |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ GRU Architecture Diagram\n",
    "\n",
    "```plaintext\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ Prev hidden  â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ Update Gate  â”‚ â† Should I keep old memory?\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ Reset Gate   â”‚ â† Should I forget past?\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ Candidate h~ â”‚ â† New memory (with reset info)\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Final h(t)  â”‚ â† Combined output (new + old)\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ğŸ§ª GRU Gates and Equations\n",
    "\n",
    "### ğŸ” 1. Update Gate â€“ How much to update memory?\n",
    "Controls how much of the previous hidden state to keep.\n",
    "\n",
    "```\n",
    "zâ‚œ = Ïƒ(W_zâ‹…[hâ‚œâ‚‹â‚, xâ‚œ] + b_z)\n",
    "\n",
    "Expend or Explain the Formula\n",
    "\n",
    "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "```\n",
    "- If zâ‚œ â‰ˆ 1: Keep old memory (maintain previous state)\n",
    "- If zâ‚œ â‰ˆ 0: Use new memory (update with current input)\n",
    "\n",
    "### ğŸ”„ 2. Reset Gate â€“ How much past to forget?\n",
    "Controls how much of the past to ignore.\n",
    "```\n",
    "râ‚œ = Ïƒ(W_râ‹…[hâ‚œâ‚‹â‚, xâ‚œ] + b_r)\n",
    "\n",
    "Expend or Explain the Formula\n",
    "\n",
    "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "```\n",
    "- If râ‚œ â‰ˆ 0: Forget past\n",
    "- If râ‚œ â‰ˆ 1: Remember all past\n",
    "\n",
    "### ğŸ§  3. Candidate Hidden State â€“ New memory to consider\n",
    "\n",
    "```\n",
    "hÌƒâ‚œ = tanh(W_hâ‹…[râ‚œâˆ—hâ‚œâ‚‹â‚, xâ‚œ] + b_h)\n",
    "\n",
    "```\n",
    "Combines current input and past memory  \n",
    "Adjusted by reset gate\n",
    "\n",
    "### ğŸ“¤ Final Hidden State â€“ The updated memory/output\n",
    "\n",
    "```\n",
    "hâ‚œ = (1âˆ’zâ‚œ)âˆ—hâ‚œâ‚‹â‚ + zâ‚œâˆ—hÌƒâ‚œ\n",
    "```\n",
    "\n",
    "Mixes old memory and new memory  \n",
    "Controlled by update gate\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Full Flow of GRU at Time Step t\n",
    "1. Take current input xâ‚œ and previous output hâ‚œâ‚‹â‚\n",
    "2. Compute update gate zâ‚œ\n",
    "3. Compute reset gate râ‚œ\n",
    "4. Calculate new memory hÌƒâ‚œ\n",
    "5. Combine using zâ‚œ to get new hidden state hâ‚œ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Real-Life Example (To Understand GRU)\n",
    "Imagine you're watching a TV drama series:\n",
    "- You remember main storylines (update gate keeps useful info)\n",
    "- You forget boring details like what color clothes someone wore (reset gate forgets unimportant parts)\n",
    "- Every new episode adds to your memory without confusing you\n",
    "\n",
    "That's how GRU works!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Where GRU is Used\n",
    "\n",
    "| Use Case | What GRU Does |\n",
    "|----------|---------------|\n",
    "| Language Translation | Remembers sentence structure |\n",
    "| Sentiment Analysis | Understands meaning in reviews |\n",
    "| Time Series Forecasting | Predicts future stock/weather |\n",
    "| Chatbots | Keeps conversation flow |\n",
    "| Speech Recognition | Understands spoken words over time |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Final Summary â€“ GRU in One Shot\n",
    "âœ… GRU is a smart RNN that learns what to remember and what to forget  \n",
    "ğŸ” Uses two gates (Update & Reset)  \n",
    "ğŸ§  Keeps a single hidden state (no cell state like LSTM)  \n",
    "âš¡ Works well for long sequences and is faster than LSTM  \n",
    "ğŸ’¬ Perfect for tasks like text, speech, and time series\n",
    "\n",
    "## **âœ… GRU Model (Single Layer) with IMDB Dataset in Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896eb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1. Load the IMDB Dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# 2. Pad Sequences to have the same length\n",
    "x_train = pad_sequences(x_train, maxlen=100)\n",
    "x_test = pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "# 3. Define the GRU Model (Single Layer)\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),  # Converts words to vectors\n",
    "    GRU(32),                                 # Single GRU Layer\n",
    "    Dense(1, activation='sigmoid')           # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# 4. Compile the Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 5. Model Summary\n",
    "model.summary()\n",
    "\n",
    "# 6. Train the Model\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
