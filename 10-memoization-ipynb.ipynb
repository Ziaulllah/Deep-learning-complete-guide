{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🧠 Memoization in MLP\n\n**Memoization** is an optimization technique used to **speed up programs** by storing the results of expensive function calls and **reusing them** when the same inputs occur again.\n\n---\n\n## 📌 Memoization in Neural Networks (MLPs)\n\nIn the context of **MLPs (Multilayer Perceptrons)** or **neural networks**, memoization is **rarely used directly** in the model’s forward pass, but it is helpful in auxiliary areas such as:\n\n- 🧠 Caching activations\n- ♻️ Avoiding recomputation\n- 💾 Storing intermediate results during backpropagation (handled by PyTorch/TensorFlow)\n\n---\n\n## 🧪 Simple Example of Memoization\n\nSuppose you have a function that performs a heavy calculation, and you want to cache the result for reuse.\n\n### ✅ Python Example (Outside MLP)\n\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef expensive_function(x):\n    print(\"Computing...\")\n    return x * x\n\nprint(expensive_function(10))  # Output: 100 (computed)\nprint(expensive_function(10))  # Output: 100 (from cache)\n```\n\n## 🤖 Memoization in MLP Training\n\nAlthough **memoization is not explicitly used** inside the forward pass of an MLP, deep learning frameworks like **PyTorch** and **TensorFlow** internally **cache intermediate values**:\n\n- ✅ **During the Forward Pass**: for activations  \n- ✅ **During the Backward Pass**: for gradient computations\n\nThis avoids **redundant calculations** and improves **training efficiency**.\n\n---\n\n### 🧠 PyTorch Example (Automatic Memoization)\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(10, 20)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(20, 1)\n\n    def forward(self, x):\n        x = self.layer1(x)  # Stored in computation graph\n        x = self.relu(x)    # Stored for gradient calculation\n        return self.layer2(x)\n```\n\nℹ️ PyTorch automatically caches intermediate tensors during the forward pass to be reused in the backward pass — similar to memoization.\n\n## ✅ Advantages of Memoization\n\n| ✅ Advantage        | 📘 Description                                                  |\n|---------------------|-----------------------------------------------------------------|\n| ⚡ Speed            | Avoids recomputing the same results repeatedly                  |\n| 🧠 Efficiency       | Saves computation time during training or prediction            |\n| 🔁 Recursive support | Helps in custom RNNs or recursive functions                     |\n| 💾 Reduced load     | Useful when inputs repeat and computations are deterministic    |\n\n---\n\n## ❌ Disadvantages of Memoization\n\n| ❌ Disadvantage           | 📘 Description                                                          |\n|---------------------------|--------------------------------------------------------------------------|\n| 🧠 Memory usage           | Requires additional memory to store cached results                       |\n| 📦 Not helpful for randomness | Doesn’t help if input values vary frequently or are stochastic           |\n| ⚠️ Limited in deep learning | Most frameworks already manage this automatically                       |\n| 🔍 Cache invalidation     | Can be complex in dynamic or data-augmented models                       |\n\n---\n\n## 🔄 When to Use Memoization with MLPs\n\n- ✅ Creating custom loss functions that reuse intermediate results  \n- ✅ Implementing recursive models  \n- ✅ Repeated forward passes in hyperparameter tuning  \n- ✅ Meta-learning or optimization algorithms (like **MAML**)\n\n---\n\n## 🧠 Summary\n\n> **Memoization ≠ Overfitting**  \n> Memoization is a **performance optimization technique** that **stores function outputs** to avoid recalculating them.\n\n✅ It is **very useful** for speeding up execution in machine learning workflows — and is often **handled automatically** by frameworks like **PyTorch** and **TensorFlow**.\n\n---\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}