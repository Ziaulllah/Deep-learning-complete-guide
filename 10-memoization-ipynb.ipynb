{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§  Memoization in MLP\n\n**Memoization** is an optimization technique used to **speed up programs** by storing the results of expensive function calls and **reusing them** when the same inputs occur again.\n\n---\n\n## ðŸ“Œ Memoization in Neural Networks (MLPs)\n\nIn the context of **MLPs (Multilayer Perceptrons)** or **neural networks**, memoization is **rarely used directly** in the modelâ€™s forward pass, but it is helpful in auxiliary areas such as:\n\n- ðŸ§  Caching activations\n- â™»ï¸ Avoiding recomputation\n- ðŸ’¾ Storing intermediate results during backpropagation (handled by PyTorch/TensorFlow)\n\n---\n\n## ðŸ§ª Simple Example of Memoization\n\nSuppose you have a function that performs a heavy calculation, and you want to cache the result for reuse.\n\n### âœ… Python Example (Outside MLP)\n\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef expensive_function(x):\n    print(\"Computing...\")\n    return x * x\n\nprint(expensive_function(10))  # Output: 100 (computed)\nprint(expensive_function(10))  # Output: 100 (from cache)\n```\n\n## ðŸ¤– Memoization in MLP Training\n\nAlthough **memoization is not explicitly used** inside the forward pass of an MLP, deep learning frameworks like **PyTorch** and **TensorFlow** internally **cache intermediate values**:\n\n- âœ… **During the Forward Pass**: for activations  \n- âœ… **During the Backward Pass**: for gradient computations\n\nThis avoids **redundant calculations** and improves **training efficiency**.\n\n---\n\n### ðŸ§  PyTorch Example (Automatic Memoization)\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(10, 20)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(20, 1)\n\n    def forward(self, x):\n        x = self.layer1(x)  # Stored in computation graph\n        x = self.relu(x)    # Stored for gradient calculation\n        return self.layer2(x)\n```\n\nâ„¹ï¸ PyTorch automatically caches intermediate tensors during the forward pass to be reused in the backward pass â€” similar to memoization.\n\n## âœ… Advantages of Memoization\n\n| âœ… Advantage        | ðŸ“˜ Description                                                  |\n|---------------------|-----------------------------------------------------------------|\n| âš¡ Speed            | Avoids recomputing the same results repeatedly                  |\n| ðŸ§  Efficiency       | Saves computation time during training or prediction            |\n| ðŸ” Recursive support | Helps in custom RNNs or recursive functions                     |\n| ðŸ’¾ Reduced load     | Useful when inputs repeat and computations are deterministic    |\n\n---\n\n## âŒ Disadvantages of Memoization\n\n| âŒ Disadvantage           | ðŸ“˜ Description                                                          |\n|---------------------------|--------------------------------------------------------------------------|\n| ðŸ§  Memory usage           | Requires additional memory to store cached results                       |\n| ðŸ“¦ Not helpful for randomness | Doesnâ€™t help if input values vary frequently or are stochastic           |\n| âš ï¸ Limited in deep learning | Most frameworks already manage this automatically                       |\n| ðŸ” Cache invalidation     | Can be complex in dynamic or data-augmented models                       |\n\n---\n\n## ðŸ”„ When to Use Memoization with MLPs\n\n- âœ… Creating custom loss functions that reuse intermediate results  \n- âœ… Implementing recursive models  \n- âœ… Repeated forward passes in hyperparameter tuning  \n- âœ… Meta-learning or optimization algorithms (like **MAML**)\n\n---\n\n## ðŸ§  Summary\n\n> **Memoization â‰  Overfitting**  \n> Memoization is a **performance optimization technique** that **stores function outputs** to avoid recalculating them.\n\nâœ… It is **very useful** for speeding up execution in machine learning workflows â€” and is often **handled automatically** by frameworks like **PyTorch** and **TensorFlow**.\n\n---\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}