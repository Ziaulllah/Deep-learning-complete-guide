{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transfer Learning**\n",
    "\n",
    "## 1. What is Transfer Learning?\n",
    "\n",
    "Transfer learning is when we **reuse a pre-trained model** (already trained on a large dataset like ImageNet) for a **new task**.  \n",
    "Instead of training from scratch, we **leverage the learned features** (edges, shapes, textures) and adapt the model for a new dataset (usually smaller).\n",
    "\n",
    "> Think of it like:  \n",
    "> *\"Borrowing someone else’s knowledge and adjusting it to solve your own problem.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why Use Transfer Learning?\n",
    "\n",
    "1. **Speeds up training** – you don’t start from zero.\n",
    "2. **Requires less data** – works even with smaller datasets.\n",
    "3. **Achieves better accuracy** – especially for small datasets.\n",
    "4. **Uses fewer computing resources.**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Steps in Transfer Learning\n",
    "\n",
    "### **Step 1: Choose a Pre-trained Model**\n",
    "- Examples: **VGG16, ResNet50, InceptionV3, MobileNet, EfficientNet**\n",
    "- These models are trained on **ImageNet (1M+ images, 1000 classes)**.\n",
    "\n",
    "### **Step 2: Load the Model**\n",
    "- Load it with or without the top classification layer:\n",
    "  - `include_top=True` → keeps the original classifier (for ImageNet).\n",
    "  - `include_top=False` → removes the classifier so you can add your own.\n",
    "\n",
    "### **Step 3: Freeze the Base Layers**\n",
    "- The convolutional layers already learned general features.\n",
    "- Set `trainable=False` so their weights don’t change.\n",
    "\n",
    "### **Step 4: Add Your Own Classifier**\n",
    "- Add Dense layers for your dataset:\n",
    "  - Example: `Flatten → Dense(128, ReLU) → Dense(num_classes, Softmax)`\n",
    "\n",
    "### **Step 5: Train the New Layers**\n",
    "- Train only the new layers for a few epochs.\n",
    "- Use a **low learning rate**.\n",
    "\n",
    "### **Step 6: Fine-Tuning (Optional)**\n",
    "- Unfreeze some of the last convolutional layers.\n",
    "- Train again with an **even lower learning rate** to adjust deeper features.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Visual Flow (Concept)\n",
    "\n",
    "1. Pre-trained model (e.g., ResNet, VGG)  \n",
    "2. Remove the old classifier  \n",
    "3. Add your custom classification head  \n",
    "4. Train the new layers  \n",
    "5. Optionally fine-tune deeper layers  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Example Code (Keras)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Load Pre-trained Model (without top layer)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Step 2: Freeze base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Step 3: Add custom classifier\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Suppose we have 10 classes\n",
    "])\n",
    "\n",
    "# Step 4: Compile\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train\n",
    "model.fit(train_data, epochs=5, validation_data=val_data)\n",
    "\n",
    "# Step 6 (Optional): Fine-tuning\n",
    "for layer in base_model.layers[-4:]:  # unfreeze last 4 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, epochs=3, validation_data=val_data)\n",
    "\n",
    "```\n",
    "\n",
    "# 6 When to Use Transfer Learning?\n",
    "\n",
    "Transfer learning is most effective when:\n",
    "\n",
    "- Your dataset is **small (less than 10,000 images)**.  \n",
    "- You need **fast training** and **don’t want to train from scratch**.  \n",
    "- Your task is **similar to the dataset the pre-trained model was trained on**,  \n",
    "  *for example: natural images → natural images*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
