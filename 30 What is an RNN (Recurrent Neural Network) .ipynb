{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“œ **Complete RNN Guide**  \n",
    "\n",
    "# ğŸ” **What is an RNN?**  \n",
    "<div style=\"background: #black; padding: 12px; border-left: 4px solid #6a5acd; border-radius: 4px;\">\n",
    "An RNN (Recurrent Neural Network) is a type of neural network designed for **sequence data** where order matters (e.g., sentences, time-series, music). Unlike standard neural networks that process inputs independently, RNNs retain memory of previous steps to process current inputs.  \n",
    "</div>\n",
    "\n",
    "### ğŸ§  **Key Idea:**  \n",
    "- Processes inputs **step-by-step**  \n",
    "- Maintains memory via a **hidden state**  \n",
    "- Predicts using both **past context** and **current input**  \n",
    "\n",
    "---\n",
    "## Why do we need RNNs?\n",
    "Standard neural networks (Feedforward/CNNs) fail with sequences because:\n",
    "- They process inputs all at once (ignores order)\n",
    "- They lack memory of previous inputs\n",
    "\n",
    "**Example**:  \n",
    "Predicting the missing word in:  \n",
    "`\"I am going to the ___.\"`  \n",
    "Requires remembering context to predict \"market\", \"school\", etc.  \n",
    "RNNs solve this by passing memory forward.\n",
    "\n",
    "---\n",
    "\n",
    "## How does an RNN work?\n",
    "Processing a sentence word-by-word:\n",
    "- At each timestep `t`:\n",
    "  1. Take current input `xâ‚œ` (e.g., a word)\n",
    "  2. Take previous hidden state `hâ‚œâ‚‹â‚` (memory)\n",
    "  3. Combine to generate new hidden state `hâ‚œ`\n",
    "  4. Produce output `yâ‚œ` (prediction)\n",
    "  5. Pass `hâ‚œ` to the next step as memory  \n",
    "Repeat for all items in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## The Formula\n",
    "### Hidden State Update:\n",
    "`hâ‚œ = tanh(Wâ‚•hâ‚œâ‚‹â‚ + Wâ‚“xâ‚œ + bâ‚•)`  \n",
    "where:\n",
    "- `hâ‚œ` = new hidden state\n",
    "- `hâ‚œâ‚‹â‚` = previous hidden state\n",
    "- `xâ‚œ` = current input\n",
    "- `Wâ‚•`, `Wâ‚“` = learned weights\n",
    "- `bâ‚•` = bias\n",
    "- `tanh` = activation (-1 to 1)\n",
    "\n",
    "### Output:\n",
    "`yâ‚œ = softmax(Wáµ§hâ‚œ + báµ§)`  \n",
    "where `yâ‚œ` = prediction at step `t`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# ğŸŒŸ **RNN Architecture Types**  \n",
    "Understand the **different ways RNNs connect inputs and outputs**.  \n",
    "These architectures define **how sequences are processed** for tasks like text, speech, and translation.  \n",
    "\n",
    "## ğŸ—ï¸ **The 4 Main Architectures**  \n",
    "\n",
    "### 1ï¸âƒ£ **One-to-One (Basic RNN)**  \n",
    "- **One input â†’ One output**  \n",
    "- Not truly sequential (acts like a normal neural network).  \n",
    "- Rare in RNN tasks.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- ğŸ–¼ï¸ Image classification (one image â†’ one label).  \n",
    "- ğŸ“Š Simple numeric prediction.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **One-to-Many (Sequence Generation)**  \n",
    "- **One input â†’ A sequence of outputs**  \n",
    "- Starts with a single input and **generates a sequence step by step**.  \n",
    "\n",
    "**How it works:**  \n",
    "- Feed one input (e.g., an image).  \n",
    "- RNN **produces multiple outputs over time**.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- ğŸ–¼ï¸ **Image Captioning:** One image â†’ full sentence.  \n",
    "- ğŸµ **Music Generation:** A style prompt â†’ a melody sequence.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Many-to-One (Sequence Classification)**  \n",
    "- **A sequence of inputs â†’ One output**  \n",
    "- RNN reads the **entire sequence** and **outputs a single prediction**.  \n",
    "\n",
    "**How it works:**  \n",
    "- Each input is processed step by step.  \n",
    "- The **final hidden state** is used to predict the result.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- ğŸ’¬ **Sentiment Analysis:** Review text â†’ Positive/Negative.  \n",
    "- ğŸ¤ **Speech Recognition (word-level):** Audio sequence â†’ Word label.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **Many-to-Many (Sequence-to-Sequence)**  \n",
    "There are **two subtypes**:  \n",
    "\n",
    "#### **(a) Synchronized (Same Length)**  \n",
    "- **Sequence input â†’ Sequence output**, **same length**.  \n",
    "- Produces output at **each time step**.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- ğŸ¥ Video frame tagging (each frame â†’ label).  \n",
    "- ğŸ·ï¸ Part-of-speech tagging (each word â†’ tag).  \n",
    "\n",
    "#### **(b) Encoderâ€“Decoder (Different Lengths)**  \n",
    "- **Sequence input â†’ Sequence output**, but **different lengths**.  \n",
    "- Uses **two RNNs**:  \n",
    "  - **Encoder:** Reads the full input and creates a **context vector**.  \n",
    "  - **Decoder:** Generates output sequence step by step.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- ğŸŒ Machine Translation (English â†’ French).  \n",
    "- ğŸ¤– Chatbots (User input â†’ AI response).  \n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“Š **Comparison Table**  \n",
    "| Architecture              | Inputs               | Outputs                | Best Used For                          |\n",
    "|---------------------------|----------------------|------------------------|----------------------------------------|\n",
    "| **One-to-One**            | Single input         | Single output          | Simple classification (images, numbers) |\n",
    "| **One-to-Many**           | Single input         | Sequence of outputs    | Captioning, music generation           |\n",
    "| **Many-to-One**           | Sequence of inputs   | Single output          | Sentiment analysis, speech recognition |\n",
    "| **Many-to-Many (Same)**   | Sequence (same len)  | Sequence (same len)    | Video tagging, part-of-speech tagging  |\n",
    "| **Many-to-Many (Encoder)**| Sequence (any length)| Sequence (any length)  | Machine translation, chatbots          |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  **Types of RNNs (Recurrent Neural Networks)**\n",
    "\n",
    "## ğŸ—ï¸ **RNN Variants Overview**\n",
    "Different RNN architectures designed to handle various sequence learning challenges.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ **Vanilla RNN (Basic RNN)**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #4682b4; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### ğŸ”§ **Basic Structure**\n",
    "`Input â†’ Hidden State â†’ Output`  \n",
    "- Processes data **step-by-step**  \n",
    "- Maintains a single hidden state  \n",
    "\n",
    "### ğŸ¯ **Best For**\n",
    "- Simple sequence tasks  \n",
    "- Next-word prediction in short sentences  \n",
    "\n",
    "### âš ï¸ **Limitations**\n",
    "- <span style=\"color: #ff6b6b;\">**Forgets long-term information**</span>  \n",
    "- Suffers from <span style=\"color: #ff6b6b;\">**vanishing gradients**</span>  \n",
    "- Struggles with long sequences  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ **LSTM (Long Short-Term Memory)**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #d6336c; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### ğŸšª **Three-Gate Architecture**\n",
    "1. **Input Gate**: Chooses new info to store  \n",
    "2. **Forget Gate**: Decides what to discard  \n",
    "3. **Output Gate**: Controls what to output  \n",
    "\n",
    "### ğŸ’ª **Advantages**\n",
    "- <span style=\"color: #Black;\">**Remembers long-term dependencies**</span>  \n",
    "- Handles vanishing gradients better  \n",
    "\n",
    "### ğŸ† **Best For**\n",
    "- Language modeling  \n",
    "- Stock market prediction  \n",
    "- Long text/speech processing  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ **GRU (Gated Recurrent Unit)**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #38a169; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### âš¡ **Simplified LSTM**\n",
    "- **Two gates instead of three**:  \n",
    "  ğŸ”„ **Update Gate** (info retention)  \n",
    "  ğŸ”„ **Reset Gate** (info forgetting)  \n",
    "\n",
    "### âš–ï¸ **Tradeoffs**\n",
    "| Pros                      | Cons                     |\n",
    "|---------------------------|--------------------------|\n",
    "| Faster training           | Slightly less powerful  |\n",
    "| Less computationally heavy| than LSTM               |\n",
    "\n",
    "### ğŸš€ **Best For**\n",
    "- Real-time applications  \n",
    "- Chatbots  \n",
    "- Speech recognition  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ **Bidirectional RNN**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #ddb892; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### ğŸ”„ **Dual Processing**\n",
    "- **Forward pass**: Past â†’ Future  \n",
    "- **Backward pass**: Future â†’ Past  \n",
    "- Combines both outputs  \n",
    "\n",
    "### ğŸ§ **When to Use**\n",
    "- When context from both directions matters  \n",
    "- Example applications:  \n",
    "  - Sentiment analysis (ğŸ˜Š/ğŸ˜)  \n",
    "  - Machine translation (ğŸŒ)  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ **Deep (Stacked) RNN**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #6c757d; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### ğŸ—ï¸ **Architecture**\n",
    "- Multiple RNN layers stacked vertically  \n",
    "- Each layer learns at different abstraction levels  \n",
    "\n",
    "### ğŸ’» **Typical Configurations**\n",
    "```python\n",
    "# Example in Keras\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True),  # First layer\n",
    "    LSTM(64),                         # Second layer\n",
    "    Dense(10)\n",
    "])\n",
    "```\n",
    "\n",
    "# âš™ï¸ **How RNNs Work**  \n",
    "### **Step-by-Step Processing:**  \n",
    "1. Take input `xâ‚œ` (e.g., a word).  \n",
    "2. Combine with previous hidden state `hâ‚œâ‚‹â‚`.  \n",
    "3. Generate new hidden state `hâ‚œ = tanh(Wâ‚•hâ‚œâ‚‹â‚ + Wâ‚“xâ‚œ + bâ‚•)`.  \n",
    "4. Produce output `yâ‚œ = softmax(Wáµ§hâ‚œ + báµ§)`.  \n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    x1[Input xâ‚] --> RNN1[RNN Cell] --> y1[Output yâ‚]\n",
    "    RNN1 --> h1((hâ‚))\n",
    "    h1 --> x2[Input xâ‚‚] --> RNN2[RNN Cell] --> y2[Output yâ‚‚]\n",
    "    RNN2 --> h2((hâ‚‚))\n",
    "    h2 --> x3[Input xâ‚ƒ] --> RNN3[RNN Cell] --> y3[Output yâ‚ƒ]\n",
    "\n",
    "```\n",
    "\n",
    "## Key Properties:\n",
    "\n",
    "**â™»ï¸ Recurrent:** Same RNN Cell reused at each step (shared weights)\n",
    "\n",
    "**ğŸ§  Memory:** Hidden state h carries forward historical information\n",
    "\n",
    "**â¡ï¸ Sequential:** Processes inputs one timestep at a time\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ğŸ§ Why Do RNNs \"Remember\"?  \n",
    "<div style=\"background: #black; padding: 12px; border-left: 4px solid #6a5acd; margin: 10px 0;\">\n",
    "\n",
    "**The Hidden State hâ‚œ is Like a Smart Notebook:**  \n",
    "ğŸ“– At each step, it _stores a compressed summary_ of all past inputs  \n",
    "ğŸ”„ The next step _updates this mental model_ to make better predictions  \n",
    "ğŸ”— Maintains context across sequences - crucial for understanding language/time-series  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## âš ï¸  Vanilla RNN Challenges  \n",
    "| Problem | Effect | Solution |  \n",
    "|---------|--------|----------|  \n",
    "| **Vanishing Gradients** | Network \"forgets\" long-term patterns | **LSTM** (Long Short-Term Memory) |  \n",
    "| **Exploding Gradients** | Unstable weight updates | **GRU** (Gated Recurrent Unit) |  \n",
    "| Limited Context Window | Fixed memory capacity | Attention Mechanisms |  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ğŸš€RNN Applications  \n",
    "<div style=\"columns: 2; column-gap: 20px;\">\n",
    "    \n",
    "â–¶ï¸ **Language Tasks**  \n",
    "- âœï¸ Creative text generation  \n",
    "- ğŸ˜Š Sentiment analysis  \n",
    "- ğŸŒ Real-time translation  \n",
    "\n",
    "â–¶ï¸ **Temporal Data**  \n",
    "- ğŸ“ˆ Stock market predictions  \n",
    "- â›… Weather forecasting  \n",
    "- ğŸµ Music composition\n",
    "\n",
    "## Powerful RNN Applications\n",
    "| Field         | Applications                          |\n",
    "|---------------|---------------------------------------|\n",
    "| NLP           | Text generation, translation, sentiment analysis |\n",
    "| Forecasting   | Stock prices, weather, sensor data    |\n",
    "| Multimedia    | Speech recognition, music generation  |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ğŸ’» Hands-On Python Example  \n",
    "```python\n",
    "# ğŸ”§ Build a Next-Word Predictor\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# ğŸ› ï¸ Model Architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10, output_dim=8, input_length=5),  # Word â†’ Vector\n",
    "    SimpleRNN(16, activation='tanh'),                      # Memory Layer\n",
    "    Dense(10, activation='softmax')                       # Prediction Layer\n",
    "])\n",
    "\n",
    "# âš™ï¸ Configure Learning\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "## This creates a simple RNN:\n",
    "\n",
    "- Takes a sequence of words.\n",
    "- Uses memory to understand context.\n",
    "- Predicts the next word.\n",
    "  \n",
    "---\n",
    "\n",
    "\n",
    "## ğŸ§© **The Storyteller's Memory (RNN Analogy)**  \n",
    "\n",
    "Imagine an RNN as someone listening to a story:  \n",
    "\n",
    "ğŸ“– **Memory Works Like:**  \n",
    "- ğŸ§  *Hidden State* = Their memory of the story so far  \n",
    "- ğŸ”Š *New Input* = Each new word they hear  \n",
    "- ğŸ”„ *Update* = Adjusting their understanding with new information  \n",
    "- ğŸ”® *Prediction* = Guessing what comes next based on context  \n",
    "\n",
    "**Just like humans:**  \n",
    "âœ… Remembers previous events (short-term memory)  \n",
    "âœ… Updates understanding with new information  \n",
    "âœ… Predicts what might happen next  \n",
    "\n",
    "*\"An RNN is like a reader who constantly updates their mental model of the story!\"*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
