{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📜 **Complete RNN Guide**  \n",
    "\n",
    "# 🔍 **What is an RNN?**  \n",
    "<div style=\"background: #black; padding: 12px; border-left: 4px solid #6a5acd; border-radius: 4px;\">\n",
    "An RNN (Recurrent Neural Network) is a type of neural network designed for **sequence data** where order matters (e.g., sentences, time-series, music). Unlike standard neural networks that process inputs independently, RNNs retain memory of previous steps to process current inputs.  \n",
    "</div>\n",
    "\n",
    "### 🧠 **Key Idea:**  \n",
    "- Processes inputs **step-by-step**  \n",
    "- Maintains memory via a **hidden state**  \n",
    "- Predicts using both **past context** and **current input**  \n",
    "\n",
    "---\n",
    "## Why do we need RNNs?\n",
    "Standard neural networks (Feedforward/CNNs) fail with sequences because:\n",
    "- They process inputs all at once (ignores order)\n",
    "- They lack memory of previous inputs\n",
    "\n",
    "**Example**:  \n",
    "Predicting the missing word in:  \n",
    "`\"I am going to the ___.\"`  \n",
    "Requires remembering context to predict \"market\", \"school\", etc.  \n",
    "RNNs solve this by passing memory forward.\n",
    "\n",
    "---\n",
    "\n",
    "## How does an RNN work?\n",
    "Processing a sentence word-by-word:\n",
    "- At each timestep `t`:\n",
    "  1. Take current input `xₜ` (e.g., a word)\n",
    "  2. Take previous hidden state `hₜ₋₁` (memory)\n",
    "  3. Combine to generate new hidden state `hₜ`\n",
    "  4. Produce output `yₜ` (prediction)\n",
    "  5. Pass `hₜ` to the next step as memory  \n",
    "Repeat for all items in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## The Formula\n",
    "### Hidden State Update:\n",
    "`hₜ = tanh(Wₕhₜ₋₁ + Wₓxₜ + bₕ)`  \n",
    "where:\n",
    "- `hₜ` = new hidden state\n",
    "- `hₜ₋₁` = previous hidden state\n",
    "- `xₜ` = current input\n",
    "- `Wₕ`, `Wₓ` = learned weights\n",
    "- `bₕ` = bias\n",
    "- `tanh` = activation (-1 to 1)\n",
    "\n",
    "### Output:\n",
    "`yₜ = softmax(Wᵧhₜ + bᵧ)`  \n",
    "where `yₜ` = prediction at step `t`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 🌟 **RNN Architecture Types**  \n",
    "Understand the **different ways RNNs connect inputs and outputs**.  \n",
    "These architectures define **how sequences are processed** for tasks like text, speech, and translation.  \n",
    "\n",
    "## 🏗️ **The 4 Main Architectures**  \n",
    "\n",
    "### 1️⃣ **One-to-One (Basic RNN)**  \n",
    "- **One input → One output**  \n",
    "- Not truly sequential (acts like a normal neural network).  \n",
    "- Rare in RNN tasks.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- 🖼️ Image classification (one image → one label).  \n",
    "- 📊 Simple numeric prediction.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **One-to-Many (Sequence Generation)**  \n",
    "- **One input → A sequence of outputs**  \n",
    "- Starts with a single input and **generates a sequence step by step**.  \n",
    "\n",
    "**How it works:**  \n",
    "- Feed one input (e.g., an image).  \n",
    "- RNN **produces multiple outputs over time**.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- 🖼️ **Image Captioning:** One image → full sentence.  \n",
    "- 🎵 **Music Generation:** A style prompt → a melody sequence.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ **Many-to-One (Sequence Classification)**  \n",
    "- **A sequence of inputs → One output**  \n",
    "- RNN reads the **entire sequence** and **outputs a single prediction**.  \n",
    "\n",
    "**How it works:**  \n",
    "- Each input is processed step by step.  \n",
    "- The **final hidden state** is used to predict the result.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- 💬 **Sentiment Analysis:** Review text → Positive/Negative.  \n",
    "- 🎤 **Speech Recognition (word-level):** Audio sequence → Word label.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ **Many-to-Many (Sequence-to-Sequence)**  \n",
    "There are **two subtypes**:  \n",
    "\n",
    "#### **(a) Synchronized (Same Length)**  \n",
    "- **Sequence input → Sequence output**, **same length**.  \n",
    "- Produces output at **each time step**.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- 🎥 Video frame tagging (each frame → label).  \n",
    "- 🏷️ Part-of-speech tagging (each word → tag).  \n",
    "\n",
    "#### **(b) Encoder–Decoder (Different Lengths)**  \n",
    "- **Sequence input → Sequence output**, but **different lengths**.  \n",
    "- Uses **two RNNs**:  \n",
    "  - **Encoder:** Reads the full input and creates a **context vector**.  \n",
    "  - **Decoder:** Generates output sequence step by step.  \n",
    "\n",
    "**Example Use Cases:**  \n",
    "- 🌐 Machine Translation (English → French).  \n",
    "- 🤖 Chatbots (User input → AI response).  \n",
    "\n",
    "---\n",
    "\n",
    "# 📊 **Comparison Table**  \n",
    "| Architecture              | Inputs               | Outputs                | Best Used For                          |\n",
    "|---------------------------|----------------------|------------------------|----------------------------------------|\n",
    "| **One-to-One**            | Single input         | Single output          | Simple classification (images, numbers) |\n",
    "| **One-to-Many**           | Single input         | Sequence of outputs    | Captioning, music generation           |\n",
    "| **Many-to-One**           | Sequence of inputs   | Single output          | Sentiment analysis, speech recognition |\n",
    "| **Many-to-Many (Same)**   | Sequence (same len)  | Sequence (same len)    | Video tagging, part-of-speech tagging  |\n",
    "| **Many-to-Many (Encoder)**| Sequence (any length)| Sequence (any length)  | Machine translation, chatbots          |\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 **Types of RNNs (Recurrent Neural Networks)**\n",
    "\n",
    "## 🏗️ **RNN Variants Overview**\n",
    "Different RNN architectures designed to handle various sequence learning challenges.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ **Vanilla RNN (Basic RNN)**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #4682b4; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### 🔧 **Basic Structure**\n",
    "`Input → Hidden State → Output`  \n",
    "- Processes data **step-by-step**  \n",
    "- Maintains a single hidden state  \n",
    "\n",
    "### 🎯 **Best For**\n",
    "- Simple sequence tasks  \n",
    "- Next-word prediction in short sentences  \n",
    "\n",
    "### ⚠️ **Limitations**\n",
    "- <span style=\"color: #ff6b6b;\">**Forgets long-term information**</span>  \n",
    "- Suffers from <span style=\"color: #ff6b6b;\">**vanishing gradients**</span>  \n",
    "- Struggles with long sequences  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ **LSTM (Long Short-Term Memory)**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #d6336c; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### 🚪 **Three-Gate Architecture**\n",
    "1. **Input Gate**: Chooses new info to store  \n",
    "2. **Forget Gate**: Decides what to discard  \n",
    "3. **Output Gate**: Controls what to output  \n",
    "\n",
    "### 💪 **Advantages**\n",
    "- <span style=\"color: #Black;\">**Remembers long-term dependencies**</span>  \n",
    "- Handles vanishing gradients better  \n",
    "\n",
    "### 🏆 **Best For**\n",
    "- Language modeling  \n",
    "- Stock market prediction  \n",
    "- Long text/speech processing  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ **GRU (Gated Recurrent Unit)**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #38a169; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### ⚡ **Simplified LSTM**\n",
    "- **Two gates instead of three**:  \n",
    "  🔄 **Update Gate** (info retention)  \n",
    "  🔄 **Reset Gate** (info forgetting)  \n",
    "\n",
    "### ⚖️ **Tradeoffs**\n",
    "| Pros                      | Cons                     |\n",
    "|---------------------------|--------------------------|\n",
    "| Faster training           | Slightly less powerful  |\n",
    "| Less computationally heavy| than LSTM               |\n",
    "\n",
    "### 🚀 **Best For**\n",
    "- Real-time applications  \n",
    "- Chatbots  \n",
    "- Speech recognition  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ **Bidirectional RNN**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #ddb892; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### 🔄 **Dual Processing**\n",
    "- **Forward pass**: Past → Future  \n",
    "- **Backward pass**: Future → Past  \n",
    "- Combines both outputs  \n",
    "\n",
    "### 🧐 **When to Use**\n",
    "- When context from both directions matters  \n",
    "- Example applications:  \n",
    "  - Sentiment analysis (😊/😞)  \n",
    "  - Machine translation (🌍)  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ **Deep (Stacked) RNN**\n",
    "<div style=\"background: #Black; padding: 12px; border-left: 4px solid #6c757d; border-radius: 4px; margin: 10px 0;\">\n",
    "\n",
    "### 🏗️ **Architecture**\n",
    "- Multiple RNN layers stacked vertically  \n",
    "- Each layer learns at different abstraction levels  \n",
    "\n",
    "### 💻 **Typical Configurations**\n",
    "```python\n",
    "# Example in Keras\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True),  # First layer\n",
    "    LSTM(64),                         # Second layer\n",
    "    Dense(10)\n",
    "])\n",
    "```\n",
    "\n",
    "# ⚙️ **How RNNs Work**  \n",
    "### **Step-by-Step Processing:**  \n",
    "1. Take input `xₜ` (e.g., a word).  \n",
    "2. Combine with previous hidden state `hₜ₋₁`.  \n",
    "3. Generate new hidden state `hₜ = tanh(Wₕhₜ₋₁ + Wₓxₜ + bₕ)`.  \n",
    "4. Produce output `yₜ = softmax(Wᵧhₜ + bᵧ)`.  \n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    x1[Input x₁] --> RNN1[RNN Cell] --> y1[Output y₁]\n",
    "    RNN1 --> h1((h₁))\n",
    "    h1 --> x2[Input x₂] --> RNN2[RNN Cell] --> y2[Output y₂]\n",
    "    RNN2 --> h2((h₂))\n",
    "    h2 --> x3[Input x₃] --> RNN3[RNN Cell] --> y3[Output y₃]\n",
    "\n",
    "```\n",
    "\n",
    "## Key Properties:\n",
    "\n",
    "**♻️ Recurrent:** Same RNN Cell reused at each step (shared weights)\n",
    "\n",
    "**🧠 Memory:** Hidden state h carries forward historical information\n",
    "\n",
    "**➡️ Sequential:** Processes inputs one timestep at a time\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 🧠Why Do RNNs \"Remember\"?  \n",
    "<div style=\"background: #black; padding: 12px; border-left: 4px solid #6a5acd; margin: 10px 0;\">\n",
    "\n",
    "**The Hidden State hₜ is Like a Smart Notebook:**  \n",
    "📖 At each step, it _stores a compressed summary_ of all past inputs  \n",
    "🔄 The next step _updates this mental model_ to make better predictions  \n",
    "🔗 Maintains context across sequences - crucial for understanding language/time-series  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ⚠️  Vanilla RNN Challenges  \n",
    "| Problem | Effect | Solution |  \n",
    "|---------|--------|----------|  \n",
    "| **Vanishing Gradients** | Network \"forgets\" long-term patterns | **LSTM** (Long Short-Term Memory) |  \n",
    "| **Exploding Gradients** | Unstable weight updates | **GRU** (Gated Recurrent Unit) |  \n",
    "| Limited Context Window | Fixed memory capacity | Attention Mechanisms |  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 🚀RNN Applications  \n",
    "<div style=\"columns: 2; column-gap: 20px;\">\n",
    "    \n",
    "▶️ **Language Tasks**  \n",
    "- ✍️ Creative text generation  \n",
    "- 😊 Sentiment analysis  \n",
    "- 🌍 Real-time translation  \n",
    "\n",
    "▶️ **Temporal Data**  \n",
    "- 📈 Stock market predictions  \n",
    "- ⛅ Weather forecasting  \n",
    "- 🎵 Music composition\n",
    "\n",
    "## Powerful RNN Applications\n",
    "| Field         | Applications                          |\n",
    "|---------------|---------------------------------------|\n",
    "| NLP           | Text generation, translation, sentiment analysis |\n",
    "| Forecasting   | Stock prices, weather, sensor data    |\n",
    "| Multimedia    | Speech recognition, music generation  |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 💻 Hands-On Python Example  \n",
    "```python\n",
    "# 🔧 Build a Next-Word Predictor\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# 🛠️ Model Architecture\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10, output_dim=8, input_length=5),  # Word → Vector\n",
    "    SimpleRNN(16, activation='tanh'),                      # Memory Layer\n",
    "    Dense(10, activation='softmax')                       # Prediction Layer\n",
    "])\n",
    "\n",
    "# ⚙️ Configure Learning\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "## This creates a simple RNN:\n",
    "\n",
    "- Takes a sequence of words.\n",
    "- Uses memory to understand context.\n",
    "- Predicts the next word.\n",
    "  \n",
    "---\n",
    "\n",
    "\n",
    "## 🧩 **The Storyteller's Memory (RNN Analogy)**  \n",
    "\n",
    "Imagine an RNN as someone listening to a story:  \n",
    "\n",
    "📖 **Memory Works Like:**  \n",
    "- 🧠 *Hidden State* = Their memory of the story so far  \n",
    "- 🔊 *New Input* = Each new word they hear  \n",
    "- 🔄 *Update* = Adjusting their understanding with new information  \n",
    "- 🔮 *Prediction* = Guessing what comes next based on context  \n",
    "\n",
    "**Just like humans:**  \n",
    "✅ Remembers previous events (short-term memory)  \n",
    "✅ Updates understanding with new information  \n",
    "✅ Predicts what might happen next  \n",
    "\n",
    "*\"An RNN is like a reader who constantly updates their mental model of the story!\"*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
