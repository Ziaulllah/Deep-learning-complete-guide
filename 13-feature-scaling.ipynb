{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## üß† What is Feature Scaling?\n\n**Feature Scaling** is the process of transforming your input features to a **similar scale**, so that no single feature **dominates** the others.\n\nWhen features have **different ranges** ‚Äî for example:\n- `age` ranges from **0 to 100**\n- `salary` ranges from **0 to 100,000**\n\n‚û°Ô∏è Machine learning models like **Neural Networks**, **KNN**, and **SVM** can **struggle to learn efficiently** without scaling.\n\n## üß™ Standardization in Feature Scaling\n\n### üìò What is Standardization?\n\n**Standardization** is a scaling technique that transforms features such that they have:\n- **Mean = 0**\n- **Standard Deviation = 1**\n\nüìå It centers the data and spreads it based on how much variance exists.  \nThis is helpful when the model assumes a **normal distribution** (like in neural networks, SVM, logistic regression, etc.).\n\n---\n\n## üß™ Example (Using `StandardScaler`)\n\n```python\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit on training data and scale it\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Scale the test data using same training parameters\nX_test_scaled = scaler.transform(X_test)\n```\n### üîç What Happens Here?\n\n#### üîπ `fit_transform(X_train)`\n\n- Calculates the **mean** and **standard deviation** of `X_train`\n- Then applies the formula:\n- **X_scaled = (X - mean) / std**\n\n\n#### üîπ `transform(X_test)`\n\n- Uses the **same mean and std** calculated from `X_train`\n- Ensures **no data leakage** from test data into training\n\n---\n\n### üìä Output Example (Standardization)\n\nIf:\n```python\nX_train = [[1], [2], [3], [4], [5]]\n```\n\n### ‚ñ∂Ô∏è Then (Result after Standardization):\n```python\nX_train_scaled = \n[[-1.41]\n [-0.71]\n [ 0.00]\n [ 0.71]\n [ 1.41]]\n```\n\n### ‚úÖ Now:\n\n- Mean ‚âà 0\n- Standard Deviation ‚âà 1\n","metadata":{}},{"cell_type":"markdown","source":"## üî¨ Normalization in Feature Scaling\n\n---\n\n### üìò What is Normalization?\n\n**Normalization** is a technique to scale input features so they all fall within a **specific range**, usually **[0, 1]** or **[-1, 1]**.\n\nüìå It‚Äôs especially useful when:\n- Features have **different units or scales**\n- You need to bring all values to a **common range**\n- You're using models that are **sensitive to distance** (like **KNN**, **SVM**, and **Neural Networks**)\n\nüìé Formula:\n**X_scaled = (X - X_min) / (X_max - X_min)**\n\n\n## üß™ Example (Using `MinMaxScaler`)\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\n# Fit and transform the training data\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Transform the test data using same scaler\nX_test_scaled = scaler.transform(X_test)\n```\n\n### üîç What Happens Here?\n\n#### üîπ `fit_transform(X_train)`\n- Finds the **minimum** and **maximum** values in `X_train`\n- Applies the formula to scale all values between 0 and 1:\n- **X_scaled = (X - min) / (max - min)**\n\n\n#### üîπ `transform(X_test)`\n- Uses the **same min and max** values from `X_train`\n- Ensures **no data leakage** into the test data\n\n---\n\n### üìä Output Example (Normalization)\n\nIf:\n```python\nX_train = [[1], [2], [3], [4], [5]]\n```\n\n### ‚ñ∂Ô∏è Then (Result after Normalization):\n\n```python\nX_train_scaled = \n[[0.00]\n [0.25]\n [0.50]\n [0.75]\n [1.00]]\n```\n\n‚úÖ **Now:**\n\n- All values are **scaled between 0 and 1**\n- The **range is normalized**, but **mean and standard deviation are not fixed**\n\n---\n\n### üéØ When to Use Normalization?\n\n| Use Case                          | ‚úÖ Use Normalization |\n|----------------------------------|----------------------|\n| Pixel/Image data (0‚Äì255)         | ‚úÖ Yes               |\n| K-Nearest Neighbors (KNN)        | ‚úÖ Yes               |\n| Support Vector Machines (SVM)    | ‚úÖ Yes               |\n| Neural Networks                  | ‚úÖ Often             |\n| Features are bounded             | ‚úÖ Best choice       |\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T20:09:22.249753Z","iopub.execute_input":"2025-07-09T20:09:22.250061Z","iopub.status.idle":"2025-07-09T20:09:22.255188Z","shell.execute_reply.started":"2025-07-09T20:09:22.250042Z","shell.execute_reply":"2025-07-09T20:09:22.254136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/content/Social_Network_Ads.csv')","metadata":{"id":"FsS0g0hH61Lk","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T20:09:22.272164Z","iopub.execute_input":"2025-07-09T20:09:22.272512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.iloc[:,2:]\ndf.head()","metadata":{"id":"jMpQva9366fP","outputId":"2dd52c89-561b-4762-ca09-24625c74393c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns","metadata":{"id":"ohok4R2Y69um","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(df.iloc[:,0],df.iloc[:,1])","metadata":{"id":"ZnBaOdwl7U2y","outputId":"da6e771b-e971-4aeb-e73d-869f8a29d0ca","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df.iloc[:,0:2]\ny = df.iloc[:,-1]","metadata":{"id":"z6bzi-427cKi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)","metadata":{"id":"YSPIQzY97i_C","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense","metadata":{"id":"Y32nU_KD7udn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(128,activation='relu',input_dim=2))\nmodel.add(Dense(1,activation='sigmoid'))","metadata":{"id":"KdFmkN8h745Y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"id":"F6xRTNvI9GMq","outputId":"fba13e1a-815d-4515-92c8-112d1e6d6cc4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","metadata":{"id":"uvLbTt9S_ByW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100)","metadata":{"id":"ZaKyBN1B_LTa","outputId":"25217972-baca-43d8-a6ea-ff8a88cd9e73","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['val_accuracy'])","metadata":{"id":"ZMQRl2lm_VHq","outputId":"4ada872e-f5c9-475d-b7d4-80bf8bac1f62","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Applying scaling","metadata":{"id":"_STlGeh5_lBW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"id":"QQGb-bfy_5TK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_scaled","metadata":{"id":"NRGauCO7KrVT","outputId":"df4a8f64-242c-4430-9cdb-2ca5da2276ab","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(X_train_scaled[:,0],X_train_scaled[:,1])","metadata":{"id":"CvhZdAbXKwoJ","outputId":"3d8b5669-434b-4b37-962e-4a0de5d1eac1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(128,activation='relu',input_dim=2))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\nhistory = model.fit(X_train_scaled,y_train,validation_data=(X_test_scaled,y_test),epochs=100)","metadata":{"id":"g-DZoervAJzJ","outputId":"8ec45eea-c1e0-463c-c9ed-77e0e75e6396","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['val_accuracy'])","metadata":{"id":"InsXWldoAVWW","outputId":"713d06f2-5f89-4818-9530-04c42cc33345","trusted":true},"outputs":[],"execution_count":null}]}