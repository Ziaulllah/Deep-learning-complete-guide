{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üîß What is a Hyperparameter?\nHyperparameters are settings you define before training a model, such as:\n\n- Number of layers\n- Number of neurons per layer\n- Learning rate\n- Batch size\n- Activation functions\n- Optimizer\n\nThese are not learned by the model ‚Äî you must manually select or automatically search for them.\n\n# üöÄ What is Keras Tuner?\nKeras Tuner helps you automatically find the best combination of hyperparameters using smart search algorithms.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport keras_tuner as kt\n\n# üì• Load and normalize MNIST\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\n# üõ†Ô∏è Build model with all tunable components\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.Flatten(input_shape=(28, 28)))\n\n    # üîÅ Tune number of hidden layers (1‚Äì3)\n    for i in range(hp.Int(\"num_layers\", 1, 3)):\n        # Dense layer with tunable units\n        model.add(layers.Dense(\n            units=hp.Int(f\"units_{i}\", min_value=64, max_value=512, step=64),\n            activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\", \"sigmoid\"])\n        ))\n\n        # Optional Batch Normalization\n        if hp.Boolean(f\"batch_norm_{i}\"):\n            model.add(layers.BatchNormalization())\n\n        # Optional Dropout\n        model.add(layers.Dropout(\n            rate=hp.Float(f\"dropout_{i}\", 0.0, 0.5, step=0.1)\n        ))\n\n    # Output layer\n    model.add(layers.Dense(10, activation=\"softmax\"))\n\n    # üîß Tune optimizer & learning rate\n    optimizer = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"])\n    learning_rate = hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n\n    # Compile the model\n    if optimizer == \"adam\":\n        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n    elif optimizer == \"rmsprop\":\n        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n    else:\n        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n\n    model.compile(\n        optimizer=opt,\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    return model\n\n# üîç Set up tuner\ntuner = kt.RandomSearch(\n    build_model,\n    objective=\"val_accuracy\",\n    max_trials=5,\n    executions_per_trial=1,\n    directory=\"my_tuner_dir\",\n    project_name=\"mnist_full_tuning\"\n)\n\n# üöÄ Start search\ntuner.search(\n    x_train, y_train,\n    epochs=5,\n    validation_split=0.2,\n    batch_size=64,\n    verbose=1  # to print Trial summary\n)\n\n# üìã Get the best trial hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(\"\\n‚úÖ Best Hyperparameters:\")\nfor param in best_hps.values:\n    print(f\"{param}: {best_hps.get(param)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T07:54:22.844730Z","iopub.execute_input":"2025-07-15T07:54:22.845154Z","iopub.status.idle":"2025-07-15T07:56:54.169055Z","shell.execute_reply.started":"2025-07-15T07:54:22.845121Z","shell.execute_reply":"2025-07-15T07:56:54.167659Z"}},"outputs":[{"name":"stdout","text":"Trial 5 Complete [00h 00m 28s]\nval_accuracy: 0.9268333315849304\n\nBest val_accuracy So Far: 0.9739166498184204\nTotal elapsed time: 00h 02m 31s\n\n‚úÖ Best Hyperparameters:\nnum_layers: 1\nunits_0: 192\nactivation_0: relu\nbatch_norm_0: True\ndropout_0: 0.30000000000000004\noptimizer: rmsprop\nlearning_rate: 0.002139602158318416\nunits_1: 64\nactivation_1: tanh\nbatch_norm_1: False\ndropout_1: 0.0\nunits_2: 128\nactivation_2: sigmoid\nbatch_norm_2: True\ndropout_2: 0.0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}