{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ§  What is a Loss Function?\n\n> A **Loss Function** tells your model **how wrong** it is by comparing the **predicted value** with the **actual value**.\n\nğŸ” The goal is to **reduce the loss** to make the model more accurate during training.\n\n---\n\n# ğŸ¤– MACHINE LEARNING LOSS FUNCTIONS\n\n---\n\n## ğŸŸ¢ ML REGRESSION LOSS FUNCTIONS\n\nUsed when the output is a **continuous value** (e.g., price, age, temperature).\n\n### ğŸ”¹ 1. Mean Squared Error (MSE)\n- ğŸ“Œ Measures average of **squared** prediction errors.\n- ğŸ” Penalizes **large errors more** than small ones.\n\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum (y - \\hat{y})^2\n\\]\n\nâœ… Use When:\n- You want high accuracy.\n- Errors should be minimized heavily.\n- Data is **clean** (no extreme outliers).\n\n---\n\n### ğŸ”¹ 2. Mean Absolute Error (MAE)\n- ğŸ“Œ Measures average of **absolute differences**.\n- ğŸ” All errors are treated **equally**, no squaring.\n\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum |y - \\hat{y}|\n\\]\n\nâœ… Use When:\n- Your data has **outliers**.\n- You want a simple and fair error measurement.\n\n---\n\n### ğŸ”¹ 3. Huber Loss\n- ğŸ“Œ Combines **MSE and MAE**.\n- ğŸ” MSE for small errors, MAE for large errors.\n\n\\[\n\\text{Huber} =\n\\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta \\cdot (|y - \\hat{y}| - \\frac{1}{2} \\delta) & \\text{otherwise}\n\\end{cases}\n\\]\n\nâœ… Use When:\n- You want the **benefits of both MSE and MAE**.\n- Data is **noisy** or contains some **outliers**.\n\n---\n\n## ğŸ”µ ML CLASSIFICATION LOSS FUNCTIONS\n\nUsed when the output is a **class label** (e.g., spam/not spam, cat/dog).\n\n### ğŸ”¹ 1. Log Loss (Binary Cross Entropy)\n- ğŸ“Œ Measures error between actual class (0 or 1) and predicted probability.\n- ğŸ” Used in **logistic regression**.\n\n\\[\n\\text{Log Loss} = - \\left[ y \\log(p) + (1 - y) \\log(1 - p) \\right]\n\\]\n\nâœ… Use When:\n- You are doing **binary classification** (2 classes).\n- You want the model to output **probabilities** (0 to 1).\n\n---\n\n### ğŸ”¹ 2. Hinge Loss\n- ğŸ“Œ Used in **Support Vector Machines (SVM)**.\n- ğŸ” Focuses on correct classification **with a margin**.\n\n\\[\n\\text{Hinge} = \\max(0, 1 - y \\cdot \\hat{y})\n\\]\n\nâœ… Use When:\n- You're using **SVM** for classification.\n- You want a model that learns **confidence margins**.\n\n---\n\n# ğŸ¤– DEEP LEARNING LOSS FUNCTIONS\n\n---\n\n## ğŸŸ¢ DL REGRESSION LOSS FUNCTIONS\n\nUsed when neural networks predict **continuous values**.\n\n### ğŸ”¹ 1. Mean Squared Error (MSE)\n- Same as ML.\n- ğŸ” Works well with **backpropagation**.\n\n### âœ… Use When:\n- You want smooth training.\n- Youâ€™re predicting numbers like prices, ages, or scores.\n\n### ğŸ§± Architecture:\n\n- Fully Connected Neural Network (FCNN)\n\n### âš¡ Activation Function:\n\n- Hidden Layers: **ReLU**\n\n- Output Layer: **Linear**\n\n---\n\n### ğŸ”¹ 2. Mean Absolute Error (MAE)\n- Same as ML.\n- ğŸ” More **robust** when training with **noisy data**.\n\n### âœ… Use When:\n- You want stable updates.\n- The dataset contains **outliers**.\n\n### ğŸ§± Architecture:\n\n- Deep Feedforward Network or FCNN\n\n### âš¡ Activation Function:\n\n- Hidden Layers: ReLU\n\n- Output Layer: Linear\n\n---\n\n### ğŸ”¹ 3. Huber Loss\n- Same as ML.\n- ğŸ” Combines smoothness of MSE and robustness of MAE.\n\nâœ… Use When:\n- Youâ€™re using **deep regression models**.\n- Data is **real-world** and may have outliers.\n\n### ğŸ§± Architecture:\n\n- Deep Regression Network / FCNN\n\n### âš¡ Activation Function:\n\n- Hidden Layers: ReLU\n\n- Output Layer: Linear\n\n  \n---\n\n## ğŸ”µ DL CLASSIFICATION LOSS FUNCTIONS\n\nUsed when output is a **class label** (binary or multi-class).\n\n### ğŸ”¹ 1. Binary Cross Entropy\n- ğŸ“Œ Same as Log Loss.\n- ğŸ” Works with **Sigmoid activation** in the final layer.\n\n\\[\n\\text{BCE} = - [y \\log(p) + (1 - y) \\log(1 - p)]\n\\]\n\n### âœ… Use When:\n- Youâ€™re predicting **2 classes** (e.g., positive/negative).\n- Final layer = **Sigmoid**.\n\n### ğŸ§± Architecture:\n\n- Binary Classification Neural Network\n\n### âš¡ Activation Function:\n\n- Hidden Layers: ReLU\n\n- Output Layer: Sigmoid\n\n---\n\n### ğŸ”¹ 2. Categorical Cross Entropy\n- ğŸ“Œ Used in **multi-class** classification.\n- ğŸ” Works with **Softmax** activation.\n-     Labels must be **one-hot encoded**.\n\n\\[\n\\text{CCE} = - \\sum y_i \\log(p_i)\n\\]\n\n### âœ… Use When:\n- You have **3+ classes**.\n- Labels are one-hot encoded.\n- Final layer = **Softmax**.\n\n### ğŸ§± Architecture:\n\n- CNN / FCNN / RNN for classification\n\n### âš¡ Activation Function:\n\n- Hidden Layers: ReLU\n\n- Output Layer: Softmax\n\n---\n\n### ğŸ”¹ 3. Sparse Categorical Cross Entropy\n- ğŸ“Œ Same as CCE, but labels are **integers**, not one-hot.\n- ğŸ” Saves **memory** and works faster with large datasets.\n\n### âœ… Use When:\n- Labels are integers like 0, 1, 2.\n- You have **many classes**.\n- Final layer = **Softmax**.\n\n### ğŸ§± Architecture:\n\n- Deep Classification Networks (CNN, RNN, Transformer)\n\n### âš¡ Activation Function:\n\n- Hidden Layers: ReLU\n\n- Output Layer: Softmax\n\n---\n\n# âœ… Final Summary\n\n| Task Type                 | ML Loss Functions             | DL Loss Functions                              |\n|---------------------------|-------------------------------|------------------------------------------------|\n| **Regression**            | MSE, MAE, Huber               | MSE, MAE, Huber                                |\n| **Binary Classification** | Log Loss, Hinge Loss          | Binary Cross Entropy                           |\n| **Multi-Class**           | â€“                             | Categorical Cross Entropy, Sparse Cross Entropy |\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}