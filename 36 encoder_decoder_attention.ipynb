{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e888bc8",
   "metadata": {},
   "source": [
    "# ğŸ”„ Encoderâ€“Decoder + Attention (Step by Step)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ğŸŒ Why Encoderâ€“Decoder?\n",
    "\n",
    "**Definition:**  \n",
    "An Encoderâ€“Decoder model is a neural network structure used for tasks like translation, summarization, and text generation. It has two parts:\n",
    "\n",
    "- Encoder reads and converts the input into hidden states.  \n",
    "- Decoder generates the output step by step.  \n",
    "\n",
    "ğŸ“Œ Example: Translate English â†’ Urdu  \n",
    "\n",
    "- Input: â€œHow are you?â€  \n",
    "- Output: â€œØ¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸâ€  \n",
    "\n",
    "We need a system that:  \n",
    "- Reads the input sentence.  \n",
    "- Understands it.  \n",
    "- Generates the output sentence.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. ğŸ§© Encoderâ€“Decoder Architecture\n",
    "\n",
    "Think of it like two friends passing information:\n",
    "\n",
    "- Encoder = Like a reader. It reads the input word by word and converts it into a summary (hidden representation).  \n",
    "- Decoder = Like a speaker. It takes that summary and produces the output sentence word by word.  \n",
    "\n",
    "ğŸ“Œ Problem: For long sentences, the encoderâ€™s summary may forget important details.  \n",
    "ğŸ‘‰ Attention solves this.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. ğŸ‘€ Attention Mechanism\n",
    "\n",
    "**Definition:**  \n",
    "The Attention mechanism allows the model to focus on the most relevant parts of the input sentence when generating each output word.  \n",
    "\n",
    "âš¡ Example:  \n",
    "- While generating â€œÛÛŒÚºâ€, the model looks mainly at â€œareâ€.  \n",
    "- While generating â€œØ¢Ù¾â€, the model looks at â€œyouâ€.  \n",
    "\n",
    "ğŸ‘‰ In short: Attention = focus on the right word at the right time.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. ğŸ¯ Bahdanau Attention (Additive Attention)\n",
    "\n",
    "**Definition:**  \n",
    "Bahdanau Attention (2014) is an additive attention mechanism that uses a small neural network to decide how much focus (weight) should be given to each input word when generating the next output word.  \n",
    "\n",
    "**Steps:**  \n",
    "1. Take the decoderâ€™s current state (s_t).  \n",
    "2. Compare it with all encoder hidden states (h1, h2, h3 â€¦).  \n",
    "3. A small NN calculates a score for each encoder hidden state.  \n",
    "4. Apply softmax â†’ scores become attention weights.  \n",
    "5. Take a weighted sum of encoder states â†’ this becomes the context vector.  \n",
    "6. Decoder uses this context vector to generate the next word.  \n",
    "\n",
    "ğŸ‘‰ In short: Bahdanau Attention = dynamic spotlight that shifts focus to different input words as output is generated.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. ğŸ¯ Luong Attention (Multiplicative Attention)\n",
    "\n",
    "**Definition:**  \n",
    "Luong Attention (2015) is a multiplicative attention mechanism that calculates the importance of input words using a simple dot product (instead of a small NN).  \n",
    "\n",
    "**Types of Luong Attention:**  \n",
    "- Dot â†’ Score = dot product of encoder state & decoder state.  \n",
    "- General â†’ Score = decoder state Ã— weight matrix Ã— encoder state.  \n",
    "- Concat â†’ Similar to Bahdanau, but less common in Luongâ€™s version.  \n",
    "\n",
    "**Steps:**  \n",
    "1. Take the decoder state and encoder states.  \n",
    "2. Use dot product (or weighted dot) to get scores.  \n",
    "3. Apply softmax â†’ attention weights.  \n",
    "4. Weighted sum â†’ context vector.  \n",
    "5. Decoder uses this vector to predict the next word.  \n",
    "\n",
    "ğŸ‘‰ In short: Luong Attention = faster than Bahdanau because it uses dot product instead of a neural network.  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. ğŸ“Š Workflow Diagram (Simple)\n",
    "\n",
    "```plaintext\n",
    "Input Sentence â†’ [ Encoder ] â†’ Hidden states â†’ Attention â†’ [ Decoder ] â†’ Output Sentence\n",
    "\n",
    "- Encoder â†’ gives hidden states for each word.\n",
    "- Attention â†’ decides which hidden states matter at each step.\n",
    "- Decoder â†’ uses them to generate the next word.\n",
    "\n",
    "```\n",
    "\n",
    "## âœ… Easy Summary\n",
    "\n",
    "- **Encoder** = Reads input sentence.\n",
    "- **Decoder** = Generates output sentence.\n",
    "- **Attention** = Helps decoder â€œlook at the right word at the right time.â€\n",
    "- **Bahdanau Attention (Additive)** = Uses a small neural network to calculate attention scores.\n",
    "- **Luong Attention (Multiplicative)** = Uses dot product (faster, simpler)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf8110",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
