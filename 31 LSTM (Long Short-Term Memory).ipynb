{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  LSTM (Long Short-Term Memory ) â€“ Complete Guide\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“œ What is LSTM?\n",
    "\n",
    "**LSTM (Long Short-Term Memory)** is a special kind of **Recurrent Neural Network (RNN)** that is able to **remember information for a long time** and forget unnecessary details.\n",
    "\n",
    "> ğŸ”¥ It solves the *vanishing gradient problem* faced by regular RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ•°ï¸ History of LSTM\n",
    "\n",
    "| Year | Milestone |\n",
    "|------|-----------|\n",
    "| **1991** | RNNs (Elman, Jordan) introduced |\n",
    "| **1997** | LSTM invented by **Hochreiter & Schmidhuber** |\n",
    "| **2015+** | Used in **Google Translate, Siri, etc.** |\n",
    "| **Today** | Core part of **NLP, speech, and time series systems** |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Why Use LSTM?\n",
    "\n",
    "### ğŸ§  RNN Limitations:\n",
    "- Struggles with **long sequences**\n",
    "- **Forgets past info**\n",
    "- Faces **vanishing gradient problem**\n",
    "\n",
    "### âœ… LSTM Advantages:\n",
    "- **Remembers long-term dependencies**\n",
    "- Uses **gates to control memory**\n",
    "- Avoids vanishing gradient issue\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” High-Level Working\n",
    "\n",
    "Imagine LSTM as a smart memory box with:\n",
    "\n",
    "1. ğŸ” **Input Gate** â€“ What to store?\n",
    "2. ğŸ§¹ **Forget Gate** â€“ What to forget?\n",
    "3. ğŸ“¤ **Output Gate** â€“ What to output?\n",
    "4. ğŸ§  **Cell State** â€“ Memory over time\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± Architecture â€“ Step by Step\n",
    "\n",
    "```plaintext\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Previous h  â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Forget Gate â”‚ â† Decides what to forget\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ Input Gate   â”‚ â† What new info to add\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Cell State  â”‚ â† Updates memory (long term)\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ Output Gate  â”‚ â† What to output this step\n",
    "        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  New h(t)    â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## ğŸ§© Components Explained\n",
    "\n",
    "### ğŸ§  1. Cell State (Câ‚œ) â€“ Long-term memory\n",
    "Like a conveyor belt carrying information\n",
    "\n",
    "Adjusted by forget & input gates\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” 2. Hidden State (hâ‚œ) â€“ Short-term memory\n",
    "Represents output at time t\n",
    "\n",
    "Passed to the next step\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Gates and Their Roles (with Equations)\n",
    "\n",
    "### ğŸ” Forget Gate â€“ What to forget?\n",
    "\n",
    "ğ‘“â‚œ = ğœ(ğ‘Šğ‘“ â‹… [â„â‚œâ‚‹â‚, ğ‘¥â‚œ] + ğ‘ğ‘“)\n",
    "\n",
    "fâ‚œ = Ïƒ(Wf â‹… [htâˆ’1, xt] + bf)\n",
    "\n",
    "Output between 0 (forget) and 1 (keep)\n",
    "\n",
    "---\n",
    "\n",
    "### âœï¸ Input Gate â€“ What to add?\n",
    "\n",
    "ğ‘–â‚œ = ğœ(ğ‘Šğ‘– â‹… [â„â‚œâ‚‹â‚, ğ‘¥â‚œ] + ğ‘ğ‘–)\n",
    "\n",
    "ğ¶Ìƒâ‚œ = tanh(ğ‘Šğ¶ â‹… [â„â‚œâ‚‹â‚, ğ‘¥â‚œ] + ğ‘ğ¶)\n",
    "\n",
    "it = Ïƒ(Wi â‹… [htâˆ’1, xt] + bi)  \n",
    "Äˆâ‚œ = tanh(WC â‹… [htâˆ’1, xt] + bC)\n",
    "\n",
    "Determines what new memory to add\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Update Cell State\n",
    "\n",
    "ğ¶â‚œ = ğ‘“â‚œ â‹… ğ¶â‚œâ‚‹â‚ + ğ‘–â‚œ â‹… ğ¶Ìƒâ‚œ\n",
    "\n",
    "Ct = ft â‹… Ctâˆ’1 + it â‹… Äˆâ‚œ\n",
    "\n",
    "Combines old memory + new info\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¤ Output Gate â€“ What to output?\n",
    "\n",
    "ğ‘œâ‚œ = ğœ(ğ‘Šğ‘œ â‹… [â„â‚œâ‚‹â‚, ğ‘¥â‚œ] + ğ‘ğ‘œ)\n",
    "\n",
    "â„â‚œ = ğ‘œâ‚œ â‹… tanh(ğ¶â‚œ)\n",
    "\n",
    "ot = Ïƒ(Wo â‹… [htâˆ’1, xt] + bo)  \n",
    "ht = ot â‹… tanh(Ct)\n",
    "\n",
    "Produces hidden/output state\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Full Flow of LSTM at Time Step t\n",
    "\n",
    "- Take input xâ‚œ and previous output hâ‚œâ‚‹â‚\n",
    "- Use Forget Gate â†’ forget some old memory\n",
    "- Use Input Gate â†’ decide new memory\n",
    "- Update Cell State Câ‚œ\n",
    "- Use Output Gate â†’ get output hâ‚œ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Real-World Use Cases\n",
    "\n",
    "| Task                | LSTM Use                    |\n",
    "|---------------------|-----------------------------|\n",
    "| Text Generation     | Predict next words          |\n",
    "| Chatbots            | Keep conversation context   |\n",
    "| Time Series         | Predict future values       |\n",
    "| Speech Recognition  | Understand entire sentence  |\n",
    "| Machine Translation | Maintain meaning through sentence |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ†š RNN vs LSTM vs GRU\n",
    "\n",
    "| Feature     | RNN  | LSTM                       | GRU               |\n",
    "|-------------|------|----------------------------|-------------------|\n",
    "| Memory      | Short| Long                       | Long              |\n",
    "| Gates       | 0    | 3 (forget, input, output)  | 2 (update, reset) |\n",
    "| Performance | Poor for long sequences | Great         | Faster            |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Final Summary (in Simple Words)\n",
    "\n",
    "- LSTM is a special RNN designed to remember useful data and forget noise\n",
    "- Uses three gates to control memory\n",
    "- Solves RNNâ€™s vanishing gradient problem\n",
    "- Great for sequence-based tasks like language, sound, and time series\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
