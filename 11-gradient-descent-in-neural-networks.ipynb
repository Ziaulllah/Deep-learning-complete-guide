{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ§  What is Gradient Descent?\n\n**Gradient Descent** is an **optimization algorithm** used to **minimize the loss (error)** in machine learning models by **updating the model's parameters** (weights and biases) in the direction that **reduces the loss**.\n\nItâ€™s like coming down a hill â€” **step by step** â€” to reach the **lowest point** (minimum error).\n\n---\n\n## ğŸ“‰ Real-Life Analogy\n\nImagine you're standing on a hill **blindfolded**, and your goal is to reach the **lowest point (valley)**.\n\nYou feel the ground with your foot, and take **small steps downward** â€”  \nâ¡ï¸ Thatâ€™s exactly what **Gradient Descent** does in ML.\n\n---\n\n## ğŸ”§ How It Works (Simple Steps)\n\n1. Start with **random weights**  \n2. Calculate the **loss** (how wrong the model is)  \n3. Find the **gradient** (slope or direction of steepest descent)  \n4. **Update weights** to reduce the loss  \n5. Repeat until the **loss is minimized**\n\n---\n\n## ğŸ“˜ Gradient Descent Formula\n\n\\[\n\\theta = \\theta - \\alpha \\cdot \\frac{\\partial J}{\\partial \\theta}\n\\]\n\nWhere:\n\n- \\( \\theta \\): parameter (weight or bias)  \n- \\( \\alpha \\): learning rate (step size)  \n- \\( \\frac{\\partial J}{\\partial \\theta} \\): gradient of the loss function with respect to parameter \\( \\theta \\)\n\n---\n\nLet me know if you'd like me to **combine all your Gradient Descent notes** into one full Markdown file for saving!\n\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# **ğŸ§  Gradient Descent in Neural Networks**\n\n**Gradient Descent** is the **engine that trains a neural network**.  \nIt adjusts the **weights & biases** to **reduce error** and make the model more **accurate**.\n\n---\n\n## ğŸ”„ How Does Gradient Descent Work in a Neural Network?\n\n### ğŸ§  Step-by-Step Process\n\n```text\nInput â¡ï¸ Forward Pass â¡ï¸ Calculate Loss â¡ï¸ Backpropagation â¡ï¸ Update Weights\n```\n--- \n\n# ğŸ” Explanation: Gradient Descent in Neural Networks\n\n## ğŸ”¹ Forward Pass\n\n- Input passes through layers  \n- Activations are calculated using current weights and biases\n\n\n\n## ğŸ”¹ Loss Function\n\n- Compares predicted output vs actual output  \n- Common examples:\n  - **MSE** (Mean Squared Error) â€“ for regression problems  \n  - **CrossEntropy** â€“ for classification problems\n\n\n\n## ğŸ”¹ Backpropagation\n\n- Calculates **gradients** of loss w.r.t. each weight using the **chain rule**  \n- Shows how much each weight contributes to the overall error\n\n\n\n## ğŸ”¹ Gradient Descent\n\n- Updates weights and biases using the computed gradients  \n- Goal is to **minimize the loss** (reach the lowest point)\n\n\n\n## ğŸ“˜ Gradient Descent Formula\n\n\\[\nw = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}\n\\]\n\n**Where:**\n- \\( w \\): weight or parameter  \n- \\( \\alpha \\): learning rate  \n- \\( \\frac{\\partial L}{\\partial w} \\): gradient of the loss function with respect to the weight\n\n---\n\n## ğŸ“Š Types of Gradient Descent (Used in Neural Networks)\n\n| Type                      | Description                                                                       |\n|---------------------------|-----------------------------------------------------------------------------------|\n| ğŸŸ¢ **Batch Gradient Descent**   | Uses the **entire training dataset** to compute gradients and update weights.     |\n| ğŸŸ¡ **Stochastic GD (SGD)**      | Uses **one data sample at a time** to compute gradients and update weights.       |\n| ğŸ”µ **Mini-Batch GD**            | Uses **small batches** of data (e.g., 32, 64, 128). This is the **most common** method. |\n\n---\n\n# **âœ… Types of Gradient Descent in Neural Networks (with PyTorch Examples)**\n\n## âœ… 1. Batch Gradient Descent\n\nUses the **entire training data** for each weight update.\n\n- âœ… High accuracy  \n- âŒ Very slow and memory intensive\n\n---\n\n### ğŸ”§ PyTorch Example\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Model\nmodel = nn.Sequential(\n    nn.Linear(2, 4),\n    nn.ReLU(),\n    nn.Linear(4, 1)\n)\n\n# Full dataset (batch)\nX = torch.randn(500, 2)  # 500 samples\ny = torch.randn(500, 1)\n\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training with full dataset in each step\nfor epoch in range(100):\n    y_pred = model(X)\n    loss = criterion(y_pred, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n\n## âœ… 2. Stochastic Gradient Descent (SGD)\n\nUses **one sample at a time** to update weights.\n\n- âœ… Faster  \n- âŒ Can be unstable or noisy\n\n### ğŸ”§ PyTorch Example\n\n```python\nfor epoch in range(100):\n    for i in range(len(X)):\n        x_sample = X[i].unsqueeze(0)  # shape (1, 2)\n        y_sample = y[i].unsqueeze(0)  # shape (1, 1)\n\n        y_pred = model(x_sample)\n        loss = criterion(y_pred, y_sample)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n```\n\n## âœ… 3. Mini-Batch Gradient Descent â­ Recommended\n\nUses small batches (e.g., 32, 64) for each update.\n\nâœ… Faster  \nâœ… More stable than full-batch or SGD\n\n\n\n### ğŸ”§ PyTorch Example\n\n```python\nbatch_size = 64\nnum_samples = X.shape[0]\n\nfor epoch in range(100):\n    for i in range(0, num_samples, batch_size):\n        x_batch = X[i:i+batch_size]\n        y_batch = y[i:i+batch_size]\n\n        y_pred = model(x_batch)\n        loss = criterion(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n--- \n\n## ğŸ“š Summary Table\n\n| Type            | Speed     | Accuracy  | Memory     | Stability       | Use Case            |\n|-----------------|-----------|-----------|------------|------------------|---------------------|\n| **Batch GD**     | âŒ Slow   | âœ… High   | âŒ High     | âœ… Very Stable   | Small datasets      |\n| **SGD**          | âœ… Fast   | âŒ Noisy  | âœ… Low      | âŒ Unstable      | Real-time training  |\n| **Mini-Batch GD**| âœ… Fast   | âœ… Good   | âœ… Moderate | âœ… Stable        | âœ… Most common       |\n\n---\n\n## ğŸ“Œ When to Use Which?\n\nâœ… Use **Mini-Batch GD** for deep learning models (**standard choice**)  \nâœ… Use **SGD** if memory is very limited or for **online learning**  \nâœ… Use **Batch GD** only for **very small datasets**\n\n---\n\n## ğŸ¯ Final Notes\n\n- All gradient descent types aim to **minimize loss**.  \n- Most frameworks (like **PyTorch** or **TensorFlow**) support **all three types**.  \n- Always **monitor your learning rate and loss curve** during training.\n","metadata":{}}]}