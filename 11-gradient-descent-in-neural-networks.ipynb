{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🧠 What is Gradient Descent?\n\n**Gradient Descent** is an **optimization algorithm** used to **minimize the loss (error)** in machine learning models by **updating the model's parameters** (weights and biases) in the direction that **reduces the loss**.\n\nIt’s like coming down a hill — **step by step** — to reach the **lowest point** (minimum error).\n\n---\n\n## 📉 Real-Life Analogy\n\nImagine you're standing on a hill **blindfolded**, and your goal is to reach the **lowest point (valley)**.\n\nYou feel the ground with your foot, and take **small steps downward** —  \n➡️ That’s exactly what **Gradient Descent** does in ML.\n\n---\n\n## 🔧 How It Works (Simple Steps)\n\n1. Start with **random weights**  \n2. Calculate the **loss** (how wrong the model is)  \n3. Find the **gradient** (slope or direction of steepest descent)  \n4. **Update weights** to reduce the loss  \n5. Repeat until the **loss is minimized**\n\n---\n\n## 📘 Gradient Descent Formula\n\n\\[\n\\theta = \\theta - \\alpha \\cdot \\frac{\\partial J}{\\partial \\theta}\n\\]\n\nWhere:\n\n- \\( \\theta \\): parameter (weight or bias)  \n- \\( \\alpha \\): learning rate (step size)  \n- \\( \\frac{\\partial J}{\\partial \\theta} \\): gradient of the loss function with respect to parameter \\( \\theta \\)\n\n---\n\nLet me know if you'd like me to **combine all your Gradient Descent notes** into one full Markdown file for saving!\n\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# **🧠 Gradient Descent in Neural Networks**\n\n**Gradient Descent** is the **engine that trains a neural network**.  \nIt adjusts the **weights & biases** to **reduce error** and make the model more **accurate**.\n\n---\n\n## 🔄 How Does Gradient Descent Work in a Neural Network?\n\n### 🧠 Step-by-Step Process\n\n```text\nInput ➡️ Forward Pass ➡️ Calculate Loss ➡️ Backpropagation ➡️ Update Weights\n```\n--- \n\n# 🔁 Explanation: Gradient Descent in Neural Networks\n\n## 🔹 Forward Pass\n\n- Input passes through layers  \n- Activations are calculated using current weights and biases\n\n\n\n## 🔹 Loss Function\n\n- Compares predicted output vs actual output  \n- Common examples:\n  - **MSE** (Mean Squared Error) – for regression problems  \n  - **CrossEntropy** – for classification problems\n\n\n\n## 🔹 Backpropagation\n\n- Calculates **gradients** of loss w.r.t. each weight using the **chain rule**  \n- Shows how much each weight contributes to the overall error\n\n\n\n## 🔹 Gradient Descent\n\n- Updates weights and biases using the computed gradients  \n- Goal is to **minimize the loss** (reach the lowest point)\n\n\n\n## 📘 Gradient Descent Formula\n\n\\[\nw = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}\n\\]\n\n**Where:**\n- \\( w \\): weight or parameter  \n- \\( \\alpha \\): learning rate  \n- \\( \\frac{\\partial L}{\\partial w} \\): gradient of the loss function with respect to the weight\n\n---\n\n## 📊 Types of Gradient Descent (Used in Neural Networks)\n\n| Type                      | Description                                                                       |\n|---------------------------|-----------------------------------------------------------------------------------|\n| 🟢 **Batch Gradient Descent**   | Uses the **entire training dataset** to compute gradients and update weights.     |\n| 🟡 **Stochastic GD (SGD)**      | Uses **one data sample at a time** to compute gradients and update weights.       |\n| 🔵 **Mini-Batch GD**            | Uses **small batches** of data (e.g., 32, 64, 128). This is the **most common** method. |\n\n---\n\n# **✅ Types of Gradient Descent in Neural Networks (with PyTorch Examples)**\n\n## ✅ 1. Batch Gradient Descent\n\nUses the **entire training data** for each weight update.\n\n- ✅ High accuracy  \n- ❌ Very slow and memory intensive\n\n---\n\n### 🔧 PyTorch Example\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Model\nmodel = nn.Sequential(\n    nn.Linear(2, 4),\n    nn.ReLU(),\n    nn.Linear(4, 1)\n)\n\n# Full dataset (batch)\nX = torch.randn(500, 2)  # 500 samples\ny = torch.randn(500, 1)\n\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training with full dataset in each step\nfor epoch in range(100):\n    y_pred = model(X)\n    loss = criterion(y_pred, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n\n## ✅ 2. Stochastic Gradient Descent (SGD)\n\nUses **one sample at a time** to update weights.\n\n- ✅ Faster  \n- ❌ Can be unstable or noisy\n\n### 🔧 PyTorch Example\n\n```python\nfor epoch in range(100):\n    for i in range(len(X)):\n        x_sample = X[i].unsqueeze(0)  # shape (1, 2)\n        y_sample = y[i].unsqueeze(0)  # shape (1, 1)\n\n        y_pred = model(x_sample)\n        loss = criterion(y_pred, y_sample)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n```\n\n## ✅ 3. Mini-Batch Gradient Descent ⭐ Recommended\n\nUses small batches (e.g., 32, 64) for each update.\n\n✅ Faster  \n✅ More stable than full-batch or SGD\n\n\n\n### 🔧 PyTorch Example\n\n```python\nbatch_size = 64\nnum_samples = X.shape[0]\n\nfor epoch in range(100):\n    for i in range(0, num_samples, batch_size):\n        x_batch = X[i:i+batch_size]\n        y_batch = y[i:i+batch_size]\n\n        y_pred = model(x_batch)\n        loss = criterion(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n--- \n\n## 📚 Summary Table\n\n| Type            | Speed     | Accuracy  | Memory     | Stability       | Use Case            |\n|-----------------|-----------|-----------|------------|------------------|---------------------|\n| **Batch GD**     | ❌ Slow   | ✅ High   | ❌ High     | ✅ Very Stable   | Small datasets      |\n| **SGD**          | ✅ Fast   | ❌ Noisy  | ✅ Low      | ❌ Unstable      | Real-time training  |\n| **Mini-Batch GD**| ✅ Fast   | ✅ Good   | ✅ Moderate | ✅ Stable        | ✅ Most common       |\n\n---\n\n## 📌 When to Use Which?\n\n✅ Use **Mini-Batch GD** for deep learning models (**standard choice**)  \n✅ Use **SGD** if memory is very limited or for **online learning**  \n✅ Use **Batch GD** only for **very small datasets**\n\n---\n\n## 🎯 Final Notes\n\n- All gradient descent types aim to **minimize loss**.  \n- Most frameworks (like **PyTorch** or **TensorFlow**) support **all three types**.  \n- Always **monitor your learning rate and loss curve** during training.\n","metadata":{}}]}