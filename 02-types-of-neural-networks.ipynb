{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **ğŸ§  Definition of Neural Network (NN)**\n\nA **Neural Network** is a set of connected nodes (like mini brain cells) that work together to understand data and make predictions.\n\nItâ€™s a tool that helps computers learn from experience, similar \nto how humans learn.\n\n**ğŸŒŸ Example:**\n\nIt can learn to recognize handwriting, voices, or even emotions in text by practicing on examples.\n\n\n## ğŸ“š Simple Explanation\n\nThink of a neural network like a **brain made up of layers of \"neurons\"**.  \nThese neurons are just **math functions** that take some input (like images, text, or numbers), process it, and pass it to the next layer.\n\n- Each layer **learns something** about the data.\n- The network keeps **adjusting itself (training)** to make better predictions.\n- Finally, it gives you a **result/output** like:\n  - `\"This is a cat\"`\n  - `\"This sentence is positive\"`\n  - `\"This person has diabetes\"`\n\n---\n\n## ğŸ§© Neural Network Structure\n\n| **Layer Type**   | **What it Does**   | **Easy Example**                  |\n|------------------|--------------------|------------------------------------|\n| Input Layer      | Takes in data      | A picture of a cat                |\n| Hidden Layer(s)  | Learns patterns    | Finds cat features like ears, eyes |\n| Output Layer     | Gives the result   | Says â€œItâ€™s a catâ€                 |\n\n---\n\n## ğŸ¯ Key Points\n\n- Neural networks **learn from data**, just like how we learn from experience.\n- They are used in **image recognition**, **language translation**, **voice assistants**, and more.\n- The more data you give, the **smarter** the network becomes.\n","metadata":{}},{"cell_type":"markdown","source":"# **ğŸ§  Types of Neural Networks (NNs)**\n\n## **ğŸ”¹ 1. Feedforward Neural Network (FNN)**\n\n### ğŸ“˜ Definition:\nThe most **basic neural network**. Data flows in **one direction** â€” from **input â†’ hidden layer(s) â†’ output**.\n\n### ğŸ”§ Use Case:\n- Digit recognition (like 0â€“9)\n- Simple classification tasks\n\nğŸ§  Think: Like a straight road â€” no loops, no turning back\n\n### ğŸ§  Easy Explanation:\n- Like water flowing through a pipe â€” it **doesnâ€™t loop back**. \n- ğŸ§  Think: Like a straight road â€” no loops, no turning back\n\n### ğŸ“Š Diagram:\n```text\nInput Layer      Hidden Layer     Output Layer\n   [X1]  â”€â”€â–¶       [O]  â”€â”€â–¶         [âœ“]\n   [X2]  â”€â”€â–¶       [O]  â”€â”€â–¶         [âœ—]\n   [X3]  â”€â”€â–¶       [O]  â”€â”€â–¶         [âœ“]\n```\n---\n","metadata":{}},{"cell_type":"markdown","source":"# **ğŸ”¹2 Multilayer Perceptron (MLP)**\n\n### ğŸ“˜ Definition:\nA **Multilayer Perceptron (MLP)** is a type of **Feedforward Neural Network** that has **multiple layers** of neurons:\n- One **Input Layer**\n- One or more **Hidden Layers**\n- One **Output Layer**\n\nEach layer is fully connected to the next layer. MLPs are trained using **backpropagation** and are widely used in deep learning tasks.\n\n### ğŸ”§ Use Case:\n- Handwritten digit recognition (like MNIST)\n- Binary and multi-class classification\n- Stock price prediction\n- Spam detection\n\n### ğŸ§  Easy Explanation:\nThink of it like an onion with layers. Data passes from one layer to another, and each layer **learns something new** about the input.\n\n### ğŸ“Š Diagram:\n```text\nInput Layer         Hidden Layers             Output Layer\n   [X1] â”€â”€â–¶           [H1]                      [O1]\n   [X2] â”€â”€â–¶           [H2]                      [O2]\n   [X3] â”€â”€â–¶           [H3]                       \n                      [H4]                       \n                      [H5]                       \n```\n---","metadata":{}},{"cell_type":"markdown","source":"## **ğŸ”¹ 2. Convolutional Neural Network (CNN)**\n\n### ğŸ“˜ Definition:\nSpecialized for **image processing**. It detects patterns like **edges, shapes, and textures** in pictures.\n\n### ğŸ”§ Use Case:\n- Image classification (e.g., Cat vs Dog)  \n- Medical imaging  \n- Face detection\n\n### ğŸ§  Easy Explanation:\nLike your eyes scanning an image **part by part**, identifying shapes.\n\n### ğŸ“Š Diagram:\n```text\n[Input Image]\n     â”‚\n[Convolution Layer]   â†’ Extract features\n     â”‚\n  [Pooling Layer]     â†’ Downsample\n     â”‚\n[Fully Connected Layer]\n     â”‚\n   [Output]\n```\n---","metadata":{}},{"cell_type":"markdown","source":"# **ğŸ”¹ 3. Recurrent Neural Network (RNN)**\n\n### ğŸ“˜ Definition:\nDesigned for **sequence data**. It has a memory of **previous inputs** to understand the current input better.\n\n### ğŸ”§ Use Case:\n- Language modeling  \n- Text generation  \n- Time-series forecasting\n\n### ğŸ§  Easy Explanation:\nLike reading a sentence â€” each word depends on the **previous ones**.\n\n### ğŸ“Š Diagram:\n```text\nInput1 â”€â”€â–¶ ğŸ§  â”€â”€â–¶ Output1\n            â”‚\nInput2 â”€â”€â–¶ ğŸ§  â”€â”€â–¶ Output2\n            â”‚\nInput3 â”€â”€â–¶ ğŸ§  â”€â”€â–¶ Output3\n\n(ğŸ§  = memory passed along the sequence)\n```\n---","metadata":{}},{"cell_type":"markdown","source":"# **ğŸ”¹ 4. Long Short-Term Memory (LSTM)**\n\n### ğŸ“˜ Definition:\nAn advanced **RNN** that can **remember important information** for a longer time and **forget irrelevant info**.\n\n### ğŸ”§ Use Case:\n- Chatbots  \n- Video analysis  \n- Stock prediction\n\n### ğŸ§  Easy Explanation:\nLike having a **notebook** â€” you remember useful things and skip the rest.\n\n### ğŸ“Š Diagram:\n```text\nInput1 â”€â”€â–¶ [LSTM Cell] â”€â”€â–¶ Output1\n               â†“\nInput2 â”€â”€â–¶ [LSTM Cell] â”€â”€â–¶ Output2\n               â†“\nInput3 â”€â”€â–¶ [LSTM Cell] â”€â”€â–¶ Output3\n```\n---","metadata":{}},{"cell_type":"markdown","source":"# **ğŸ”¹ 5. Generative Adversarial Network (GAN)**\n\n### ğŸ“˜ Definition:\nA system of **two networks**:\n- **Generator:** Creates fake data  \n- **Discriminator:** Detects whether itâ€™s real or fake\n\n### ğŸ”§ Use Case:\n- Deepfake generation  \n- Image-to-image translation  \n- AI-generated art\n\n### ğŸ§  Easy Explanation:\nLike a **forger and a detective** â€” both improve by trying to beat each other.\n\n### ğŸ“Š Diagram:\n```text\n[Random Noise]\n      â”‚\n  ğŸ¨ Generator â”€â”€â”€â”€â”€â–¶ Fake Image â”€â”€â”€â”€â”€â–¶ ğŸ¤– Discriminator\n                                        â”‚\n                                  Real or Fake?\n```\n---","metadata":{}},{"cell_type":"markdown","source":"# **ğŸ”¹ 6. Radial Basis Function Network (RBF)**\n\n### ğŸ“˜ Definition:\nUses the **distance** between input data and predefined center points to make decisions.\n\n### ğŸ”§ Use Case:\n- Pattern recognition  \n- Classification tasks  \n- Function approximation\n\n### ğŸ§  Easy Explanation:\nLike **grouping people** by how close they are to known examples.\n\n### ğŸ“Š Diagram:\n```text\n[Input] â”€â”€â–¶ [RBF Layer] â”€â”€â–¶ [Output]\n              â”‚\n     (Check distance to known points)\n```\n---","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}