{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🧠 What is Deep Learning?\n\nDeep Learning is a subfield of **Machine Learning** that uses **artificial neural networks** to mimic how the **human brain learns** from data.\n\nIt powers:\n\n- Voice Assistants like Siri, Alexa  \n- Self-driving cars  \n- Face Recognition  \n- ChatGPT (and similar models)  \n- Google Translate  \n\n---\n\n# 📚History of Deep Learning \n\n## 🔹 1943 – First Artificial Neuron  \n**Scientists:** Warren McCulloch & Walter Pitts  \n**Country:** 🇺🇸 United States  \n\n**🔍 What happened?**  \nThey developed the first mathematical model of a neuron (McCulloch-Pitts Neuron).\n\n**📌 Why it matters:**  \nThis model became the **foundation of neural networks** — showing that a brain-like system could be created using math.\n\n---\n\n## 🔹 1958 – The Perceptron  \n**Scientist:** Frank Rosenblatt  \n**Country:** 🇺🇸 United States  \n\n**🔍 What happened?**  \nHe introduced the **Perceptron**, the first model that could **learn using weights**.\n\n**📌 Why it matters:**  \n- First **trainable neural network**  \n- But it couldn’t solve the **XOR problem** (a logic puzzle)\n\n---\n\n## 🔹 1969 – Neural Networks Dismissed  \n**Scientists:** Marvin Minsky & Seymour Papert  \n**Country:** 🇺🇸 United States  \n\n**🔍 What happened?**  \nThey published a book showing the Perceptron couldn’t solve **non-linear problems** like XOR.\n\n**📌 Why it matters:**  \nIt caused the first **AI Winter** — research and funding dropped for years.\n\n---\n\n## 🔹 1986 – Backpropagation Saves Neural Nets  \n**Scientists:** Geoffrey Hinton (🇨🇦), David Rumelhart (🇺🇸), Ronald Williams (🇺🇸)  \n\n**🔍 What happened?**  \nThey introduced **Backpropagation**, a method to train **multilayer networks**.\n\n**📌 Why it matters:**  \nDeep networks could now **learn from their errors** → more powerful models.\n\n---\n\n## 🔹 1998 – LeNet-5: First CNN  \n**Scientist:** Yann LeCun  \n**Country:** 🇫🇷 France (working in 🇺🇸 USA at Bell Labs)  \n\n**🔍 What happened?**  \nHe developed **LeNet-5**, a CNN for recognizing handwritten digits.\n\n**📌 Why it matters:**  \nIt proved **CNNs work well for image recognition**, used by **banks** to read checks.\n\n---\n\n## 🔹 2006 – Deep Belief Networks  \n**Scientist:** Geoffrey Hinton  \n**Country:** 🇨🇦 Canada  \n\n**🔍 What happened?**  \nHe introduced **Deep Belief Networks** with unsupervised pretraining.\n\n**📌 Why it matters:**  \nSolved the problem of **training deep networks** and marked the **revival of deep learning**.\n\n---\n\n## 🔹 2012 – AlexNet Wins ImageNet  \n**Scientists:** Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton  \n**Country:** 🇨🇦 Canada  \n\n**🔍 What happened?**  \nThey built **AlexNet**, a deep CNN that **won the ImageNet competition**.\n\n**📌 Why it matters:**  \nProved deep learning + GPU training was **more powerful than traditional methods**.  \nMade deep learning **mainstream**.\n\n---\n\n## 🔹 2014 – GANs Introduced  \n**Scientist:** Ian Goodfellow  \n**Country:** 🇺🇸 United States  \n\n**🔍 What happened?**  \nHe introduced **GANs (Generative Adversarial Networks)** — Generator vs Discriminator.\n\n**📌 Why it matters:**  \nGANs can create **realistic images, deepfakes, and AI art**.\n\n---\n\n## 🔹 2017 – Transformers Revolution  \n**Scientists:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, et al.  \n**Country:** 🇺🇸 USA (Google Brain)  \n\n**🔍 What happened?**  \nThey introduced **Transformers** in the paper: _\"Attention is All You Need\"_.\n\n**📌 Why it matters:**  \nTransformers became the base of modern NLP models like:\n- **BERT** (Google 🇺🇸)  \n- **GPT series** (OpenAI 🇺🇸)\n\n---\n\n## 🔹 2020s – Foundation Models & Generative AI  \n**Organizations:**\n\n- OpenAI 🇺🇸 — GPT-3, GPT-4  \n- Google DeepMind 🇬🇧 — PaLM, Gemini  \n- Meta AI 🇺🇸 — LLaMA  \n- Anthropic 🇺🇸 — Claude  \n\n**📌 Why it matters:**  \nThese **LLMs (Large Language Models)** can:\n\n- Generate text  \n- Write code  \n- Summarize documents  \n- Chat like humans  \n\nUsed in **ChatGPT, Bard, Claude, LLaMA**, and more.\n\n---\n\n# ✅ Final Timeline Summary\n\n| Year  | Milestone                 | Scientist / Team              | Country        | Why It Matters                              |\n|-------|---------------------------|-------------------------------|----------------|----------------------------------------------|\n| 1943  | Artificial Neuron         | McCulloch & Pitts             | 🇺🇸 USA         | First brain-like model                       |\n| 1958  | Perceptron                | Frank Rosenblatt              | 🇺🇸 USA         | First learnable network                      |\n| 1969  | AI Winter starts          | Minsky & Papert               | 🇺🇸 USA         | Showed limits of neural nets                 |\n| 1986  | Backpropagation           | Hinton, Rumelhart, Williams   | 🇨🇦 🇺🇸          | Trained deep networks                        |\n| 1998  | LeNet-5 (CNN)             | Yann LeCun                    | 🇫🇷 🇺🇸          | First CNN for digit recognition              |\n| 2006  | Deep Belief Networks      | Geoffrey Hinton               | 🇨🇦 Canada      | Restarted deep learning interest             |\n| 2012  | AlexNet wins ImageNet     | Krizhevsky, Sutskever, Hinton | 🇨🇦 Canada      | Made DL popular with GPU training            |\n| 2014  | GANs introduced           | Ian Goodfellow                | 🇺🇸 USA         | Fake image generation starts                 |\n| 2017  | Transformers              | Google Brain team             | 🇺🇸 USA         | Revolutionized language understanding        |\n| 2020+ | GPT, BERT, LLaMA, etc.    | OpenAI, Google, Meta, etc.    | 🇺🇸 🇬🇧           | Foundation models used in real-world AI      |\n\n---\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}