{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ§  What is Deep Learning?\n\nDeep Learning is a subfield of **Machine Learning** that uses **artificial neural networks** to mimic how the **human brain learns** from data.\n\nIt powers:\n\n- Voice Assistants like Siri, Alexa  \n- Self-driving cars  \n- Face Recognition  \n- ChatGPT (and similar models)  \n- Google Translate  \n\n---\n\n# ğŸ“šHistory of Deep Learning \n\n## ğŸ”¹ 1943 â€“ First Artificial Neuron  \n**Scientists:** Warren McCulloch & Walter Pitts  \n**Country:** ğŸ‡ºğŸ‡¸ United States  \n\n**ğŸ” What happened?**  \nThey developed the first mathematical model of a neuron (McCulloch-Pitts Neuron).\n\n**ğŸ“Œ Why it matters:**  \nThis model became the **foundation of neural networks** â€” showing that a brain-like system could be created using math.\n\n---\n\n## ğŸ”¹ 1958 â€“ The Perceptron  \n**Scientist:** Frank Rosenblatt  \n**Country:** ğŸ‡ºğŸ‡¸ United States  \n\n**ğŸ” What happened?**  \nHe introduced the **Perceptron**, the first model that could **learn using weights**.\n\n**ğŸ“Œ Why it matters:**  \n- First **trainable neural network**  \n- But it couldnâ€™t solve the **XOR problem** (a logic puzzle)\n\n---\n\n## ğŸ”¹ 1969 â€“ Neural Networks Dismissed  \n**Scientists:** Marvin Minsky & Seymour Papert  \n**Country:** ğŸ‡ºğŸ‡¸ United States  \n\n**ğŸ” What happened?**  \nThey published a book showing the Perceptron couldnâ€™t solve **non-linear problems** like XOR.\n\n**ğŸ“Œ Why it matters:**  \nIt caused the first **AI Winter** â€” research and funding dropped for years.\n\n---\n\n## ğŸ”¹ 1986 â€“ Backpropagation Saves Neural Nets  \n**Scientists:** Geoffrey Hinton (ğŸ‡¨ğŸ‡¦), David Rumelhart (ğŸ‡ºğŸ‡¸), Ronald Williams (ğŸ‡ºğŸ‡¸)  \n\n**ğŸ” What happened?**  \nThey introduced **Backpropagation**, a method to train **multilayer networks**.\n\n**ğŸ“Œ Why it matters:**  \nDeep networks could now **learn from their errors** â†’ more powerful models.\n\n---\n\n## ğŸ”¹ 1998 â€“ LeNet-5: First CNN  \n**Scientist:** Yann LeCun  \n**Country:** ğŸ‡«ğŸ‡· France (working in ğŸ‡ºğŸ‡¸ USA at Bell Labs)  \n\n**ğŸ” What happened?**  \nHe developed **LeNet-5**, a CNN for recognizing handwritten digits.\n\n**ğŸ“Œ Why it matters:**  \nIt proved **CNNs work well for image recognition**, used by **banks** to read checks.\n\n---\n\n## ğŸ”¹ 2006 â€“ Deep Belief Networks  \n**Scientist:** Geoffrey Hinton  \n**Country:** ğŸ‡¨ğŸ‡¦ Canada  \n\n**ğŸ” What happened?**  \nHe introduced **Deep Belief Networks** with unsupervised pretraining.\n\n**ğŸ“Œ Why it matters:**  \nSolved the problem of **training deep networks** and marked the **revival of deep learning**.\n\n---\n\n## ğŸ”¹ 2012 â€“ AlexNet Wins ImageNet  \n**Scientists:** Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton  \n**Country:** ğŸ‡¨ğŸ‡¦ Canada  \n\n**ğŸ” What happened?**  \nThey built **AlexNet**, a deep CNN that **won the ImageNet competition**.\n\n**ğŸ“Œ Why it matters:**  \nProved deep learning + GPU training was **more powerful than traditional methods**.  \nMade deep learning **mainstream**.\n\n---\n\n## ğŸ”¹ 2014 â€“ GANs Introduced  \n**Scientist:** Ian Goodfellow  \n**Country:** ğŸ‡ºğŸ‡¸ United States  \n\n**ğŸ” What happened?**  \nHe introduced **GANs (Generative Adversarial Networks)** â€” Generator vs Discriminator.\n\n**ğŸ“Œ Why it matters:**  \nGANs can create **realistic images, deepfakes, and AI art**.\n\n---\n\n## ğŸ”¹ 2017 â€“ Transformers Revolution  \n**Scientists:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, et al.  \n**Country:** ğŸ‡ºğŸ‡¸ USA (Google Brain)  \n\n**ğŸ” What happened?**  \nThey introduced **Transformers** in the paper: _\"Attention is All You Need\"_.\n\n**ğŸ“Œ Why it matters:**  \nTransformers became the base of modern NLP models like:\n- **BERT** (Google ğŸ‡ºğŸ‡¸)  \n- **GPT series** (OpenAI ğŸ‡ºğŸ‡¸)\n\n---\n\n## ğŸ”¹ 2020s â€“ Foundation Models & Generative AI  \n**Organizations:**\n\n- OpenAI ğŸ‡ºğŸ‡¸ â€” GPT-3, GPT-4  \n- Google DeepMind ğŸ‡¬ğŸ‡§ â€” PaLM, Gemini  \n- Meta AI ğŸ‡ºğŸ‡¸ â€” LLaMA  \n- Anthropic ğŸ‡ºğŸ‡¸ â€” Claude  \n\n**ğŸ“Œ Why it matters:**  \nThese **LLMs (Large Language Models)** can:\n\n- Generate text  \n- Write code  \n- Summarize documents  \n- Chat like humans  \n\nUsed in **ChatGPT, Bard, Claude, LLaMA**, and more.\n\n---\n\n# âœ… Final Timeline Summary\n\n| Year  | Milestone                 | Scientist / Team              | Country        | Why It Matters                              |\n|-------|---------------------------|-------------------------------|----------------|----------------------------------------------|\n| 1943  | Artificial Neuron         | McCulloch & Pitts             | ğŸ‡ºğŸ‡¸ USA         | First brain-like model                       |\n| 1958  | Perceptron                | Frank Rosenblatt              | ğŸ‡ºğŸ‡¸ USA         | First learnable network                      |\n| 1969  | AI Winter starts          | Minsky & Papert               | ğŸ‡ºğŸ‡¸ USA         | Showed limits of neural nets                 |\n| 1986  | Backpropagation           | Hinton, Rumelhart, Williams   | ğŸ‡¨ğŸ‡¦ ğŸ‡ºğŸ‡¸          | Trained deep networks                        |\n| 1998  | LeNet-5 (CNN)             | Yann LeCun                    | ğŸ‡«ğŸ‡· ğŸ‡ºğŸ‡¸          | First CNN for digit recognition              |\n| 2006  | Deep Belief Networks      | Geoffrey Hinton               | ğŸ‡¨ğŸ‡¦ Canada      | Restarted deep learning interest             |\n| 2012  | AlexNet wins ImageNet     | Krizhevsky, Sutskever, Hinton | ğŸ‡¨ğŸ‡¦ Canada      | Made DL popular with GPU training            |\n| 2014  | GANs introduced           | Ian Goodfellow                | ğŸ‡ºğŸ‡¸ USA         | Fake image generation starts                 |\n| 2017  | Transformers              | Google Brain team             | ğŸ‡ºğŸ‡¸ USA         | Revolutionized language understanding        |\n| 2020+ | GPT, BERT, LLaMA, etc.    | OpenAI, Google, Meta, etc.    | ğŸ‡ºğŸ‡¸ ğŸ‡¬ğŸ‡§           | Foundation models used in real-world AI      |\n\n---\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}