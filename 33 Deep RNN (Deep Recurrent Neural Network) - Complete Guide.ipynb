{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5bd31e",
   "metadata": {},
   "source": [
    "# ðŸ§  Deep RNN (Deep Recurrent Neural Network) - Complete Guide\n",
    "\n",
    "## ðŸ“œ What is a Deep RNN?\n",
    "A Deep RNN is just like a normal RNN, but with multiple layers stacked on top of each other.\n",
    "\n",
    "- ðŸ§  Itâ€™s like a multi-layer brain that learns more complex - - patterns in sequence data.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Multi-layer architecture (typically 3-10 layers)\n",
    "- Each layer processes the sequence at different abstraction levels\n",
    "- Maintains temporal connections across layers\n",
    "\n",
    "## ðŸ¤” Why Do We Need \"Deep\" RNNs?\n",
    "A basic (shallow) RNN has only one layer, which may not be enough to:\n",
    "\n",
    "- Learn complex language or patterns\n",
    "- Understand deep context in sequences\n",
    "- Capture both low-level and high-level features\n",
    "\n",
    "So, we stack multiple RNN layers to build a Deep RNN â€” making it more powerful.\n",
    "\n",
    "## ðŸ§  Layered Processing Analogy\n",
    "| Layer | Function | Detective Analogy |\n",
    "|-------|----------|-------------------|\n",
    "| Layer 1 | Low-level feature extraction | ðŸ•µï¸ Identifying basic clues (words, phonemes) |\n",
    "| Layer 2 | Pattern recognition | ðŸ” Connecting clues (phrases, intonation) |\n",
    "| Layer 3+ | High-level understanding | ðŸ§  Solving the mystery (meaning, intent) |\n",
    "\n",
    "## ðŸ§± Architecture Overview\n",
    "```plaintext\n",
    "Time Step t-1       Time Step t       Time Step t+1\n",
    "     â†“                  â†“                  â†“\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " â”‚ Input  â”‚        â”‚ Input  â”‚        â”‚ Input  â”‚\n",
    " â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚                  â”‚                  â”‚\n",
    " â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    " â”‚ RNN L1 â”‚        â”‚ RNN L1 â”‚        â”‚ RNN L1 â”‚ â† Feature Extraction\n",
    " â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚                  â”‚                  â”‚\n",
    " â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    " â”‚ RNN L2 â”‚        â”‚ RNN L2 â”‚        â”‚ RNN L2 â”‚ â† Pattern Recognition\n",
    " â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚                  â”‚                  â”‚\n",
    " â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    " â”‚Output  â”‚        â”‚Output  â”‚        â”‚Output  â”‚\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    " ```\n",
    "## âš™ï¸ How Deep RNNs Work\n",
    "\n",
    "### 1. Input Processing\n",
    "- Sequential data flows through the network at each timestep `t`\n",
    "- For each layer `l`, receives:\n",
    "  - Input from previous layer: `h_t^{l-1}`\n",
    "  - Hidden state from previous timestep: `h_{t-1}^l`\n",
    "\n",
    "```plaintext\n",
    "           [Input]\n",
    "              â†“\n",
    "Timestep t-1 â†’ [RNN Layer 1] â†’ [RNN Layer 2] â†’ Output\n",
    "              â†“        â†‘        â†“        â†‘\n",
    "Timestep t   â†’ [RNN Layer 1] â†’ [RNN Layer 2] â†’ Output\n",
    "\n",
    "## âš™ï¸ Layer-wise Propagation in Deep RNNs\n",
    "```\n",
    "\n",
    "### Core Equation / Layer-wise Propagation\n",
    "- At each timestep `t` and layer `l`, the hidden state **updates as:**\n",
    "```\n",
    "h_t^l = f(W^l[h_t^{l-1}, h_{t-1}^l] + b^l)\n",
    "```\n",
    "### Output Generation:\n",
    "- Final layer produces predictions or representations\n",
    "In Simple word\n",
    "- Input at time t goes to the first RNN layer\n",
    "- The output from the first layer becomes the input to the - second RNN layer\n",
    "- This continues through all layers\n",
    "- The final layer produces the output\n",
    "\n",
    "## Below is a complete and easy code example of:\n",
    "\n",
    "- **âœ… Deep RNN**\n",
    "- **âœ… Deep LSTM**\n",
    "- **âœ… Deep GRU**\n",
    "\n",
    "We'll use Tensorflow/Keras (the most student-friendly and readable deep learning framework) to build these models step-by-step.\n",
    "\n",
    "## **âœ… 1. Deep RNN with IMDB (2 Layers)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba27d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the IMDB Dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Pad sequences to same length\n",
    "x_train = pad_sequences(x_train, maxlen=100)\n",
    "x_test = pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "# Build Deep RNN model\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),    # Embedding Layer\n",
    "    SimpleRNN(32, return_sequences=True),      # 1st RNN Layer\n",
    "    SimpleRNN(16),                             # 2nd RNN Layer\n",
    "    Dense(1, activation='sigmoid')             # Output for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28572e21",
   "metadata": {},
   "source": [
    "## **ðŸ§  2. Deep LSTM with IMDB (2 Layers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab41608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    LSTM(32, return_sequences=True),   # 1st LSTM Layer\n",
    "    LSTM(16),                          # 2nd LSTM Layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869cbc8",
   "metadata": {},
   "source": [
    "## **âš¡ 3. Deep GRU with IMDB (2 Layers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b454b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    GRU(32, return_sequences=True),    # 1st GRU Layer\n",
    "    GRU(16),                           # 2nd GRU Layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb6cf7",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Model Architecture Comparison\n",
    "\n",
    "| Model       | Layer 1              | Layer 2              | Output Layer | Parameters | Typical Use Cases               |\n",
    "|-------------|----------------------|----------------------|--------------|------------|----------------------------------|\n",
    "| **Deep RNN**  | `SimpleRNN(32)`      | `SimpleRNN(16)`      | `Dense(1)`   | ~1-3K      | Basic sequence prediction        |\n",
    "|             | (tanh activation)    | (tanh activation)    | (linear)     |            |                                  |\n",
    "| **Deep LSTM** | `LSTM(32)`          | `LSTM(16)`          | `Dense(1)`   | ~10-15K    | Long-term dependency tasks       |\n",
    "|             | (with forget gate)   | (with peepholes)     | (sigmoid)    |            | (e.g., speech recognition)       |\n",
    "| **Deep GRU**  | `GRU(32)`           | `GRU(16)`           | `Dense(1)`   | ~7-12K     | Memory-efficient applications    |\n",
    "|             | (reset/update gates)| (reset/update gates)| (softmax)    |            | (e.g., real-time predictions)    |\n",
    "\n",
    "### Key Characteristics:\n",
    "1. **Parameter Complexity**:\n",
    "   - LSTM > GRU > RNN (for same hidden units)\n",
    "   \n",
    "2. **Memory Mechanisms**:\n",
    "   ```mermaid\n",
    "   graph LR\n",
    "   A[RNN] -->|Single hidden state| B[Basic memory]\n",
    "   C[LSTM] -->|Cell state + 3 gates| D[Long-term memory]\n",
    "   E[GRU] -->|Hidden state + 2 gates| F[Adaptive memory]\n",
    "   ```\n",
    "3. **Performance Trade-offs**:\n",
    "\n",
    "- **RNN:** Fastest but prone to vanishing gradients\n",
    "- **LSTM:** Best for long sequences but computationally heavy\n",
    "- **GRU:** Balanced approach with fewer parameters than LSTM\n",
    "\n",
    "ðŸ’¡ Pro Tip: For most modern applications, Deep GRU architectures provide the best balance between performance and computational efficiency."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
