{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ“œ HISTORY OF CNN\n\n## ðŸ”¶ 1. 1980 â€” Neocognitron\n**Invented by:** A Japanese scientist Kunihiko Fukushima  \n\n**What it did:**  \n- Inspired by how the human brain recognizes shapes  \n- Early form of CNN â€” used for character recognition (like handwritten digits)  \n\n**Limitation:**  \n- No training using backpropagation  \n\n---\n\n## ðŸ”¶ 2. 1989 â€” Backpropagation in CNN  \n**Researchers:** Yann LeCun et al.  \n\n**Contribution:**  \n- Combined convolutional layers with backpropagation  \n- Allowed the network to learn automatically from data  \n- Used for: Simple image recognition tasks  \n\n---\n\n## ðŸ”¶ 3. 1998 â€” LeNet-5  \n**Invented by:** Yann LeCun  \n\n**What it did:**  \n- Recognized handwritten digits (like in ZIP codes)  \n\n**Architecture:**  \n- 7 layers (including convolution and pooling)  \n- Used ReLU, Sigmoid, Tanh activations  \n\nðŸ“Œ **Important Moment:** First successful CNN model for real-world use  \n\n---\n\n## ðŸ”¶ 4. 2006 â€” Deep Learning Era Begins  \n- Hinton and team introduced Deep Belief Networks (DBNs)  \n- Paved the way for deeper CNNs by pretraining layers  \n- Solved training problems of deep networks  \n\n---\n\n## ðŸ”¶ 5. 2012 â€” AlexNet  \n**Invented by:** Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton  \n\n**Event:** Won ImageNet competition by a huge margin  \n\n**Breakthroughs:**  \n- Used ReLU for activation  \n- Trained on GPU (faster!)  \n- Used dropout to avoid overfitting  \n\nðŸ“Œ This is the moment CNNs became famous worldwide  \n\n---\n\n## ðŸ”¶ 6. 2014â€“2015 â€” VGG, GoogleNet, ResNet  \n\n| Model       | Key Feature                          |\n|-------------|-------------------------------------|\n| VGGNet      | Deep but simple (many 3x3 filters)  |\n| GoogleNet   | Inception modules (multi-path filters) |\n| ResNet      | Skip connections (solves vanishing gradient) |\n\n---\n\n## ðŸ”¶ 7. 2016â€“2020 â€” Faster Models  \n- **YOLO, SSD:** Real-time object detection using CNN  \n- **MobileNet, EfficientNet:** Lightweight CNNs for mobile/edge devices  \n\n---\n\n## ðŸ”¶ 8. 2020â€“Now â€” CNNs vs Transformers  \n- CNNs are still widely used  \n- Transformers (like ViT) started competing for image tasks  \n- CNNs still dominate in real-time systems (like YOLO)  \n\n---\n\n## ðŸ“Œ Summary Timeline  \n\n| Year  | Model/Event       | Contribution                          |\n|-------|-------------------|---------------------------------------|\n| 1980  | Neocognitron      | CNN concept introduced                |\n| 1989  | Backpropagation   | Learning ability added                |\n| 1998  | LeNet-5           | First successful CNN                  |\n| 2012  | AlexNet           | Deep CNN + GPU = breakthrough        |\n| 2014  | VGGNet            | Simple, deep architecture            |\n| 2015  | ResNet            | Solved deep network issues           |\n| 2016+ | YOLO, SSD         | Real-time object detection           |","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}